





\section{Proof of Theorem~\ref{thm:euler}}

Before proceeding to the proof, let us first define the following Euler-Maruyama scheme which will be useful for our analysis:
\begin{align}
\hat{X}_{k+1}  = \hat{X}_k + h \hat{v}(\hat{X}_k, \mu_{kh}) + \sqrt{2\lambda h}Z_{n+1},
\end{align}
where $\mu_t$ denotes the probability distribution of $X_t$ with $(X_t)_t$ being the solution of the original SDE \eqref{eqn:sde}. Now, consider the probability distribution of $\hat{X}_k$ as $\muh_{kh}$.  Starting from the discrete-time process $(\hat{X}_k)_{k\in \mathbb{N}_+}$, we first define a continuous-time process $(Y_t)_{t\geq 0}$ that linearly interpolates $(\hat{X}_k)_{k\in \mathbb{N}_+}$, given as follows: 
\begin{align}
d Y_t = \tilde{v}_t(Y) dt + \sqrt{2 \lambda} dW_t, \label{eqn:sde_linear}
\end{align}
where $\tilde{v}_t(Y) \triangleq - \sum_{k=0}^{\infty} \hat{v}_{kh} (Y_{kh}) \mathds{1}_{[kh, (k+1)h)}(t)$ and $\mathds{1}$ denotes the indicator function.
Similarly, we define a continuous-time process $(U_t)_{t\geq 0}$ that linearly interpolates $(\bar{X}_k)_{k\in \mathbb{N}_+}$, defined by \eqref{eqn:euler_asymp}, given as follows: 
\begin{align}
d U_t = \bar{v}_t(U) dt + \sqrt{2 \lambda} dW_t, \label{eqn:sde_linear2}
\end{align}
where
$\bar{v}_t(U) \triangleq - \sum_{k=0}^{\infty} \hat{v} (U_{kh},
\mub_{kh}) \mathds{1}_{[kh, (k+1)h)}(t)$ and $\mub_{kh}$ denotes the
probability distribution of $\bar{X}_k$.  Let us denote the
distributions of $(X_t)_{t \in [0,T]}$, $(Y_t)_{t \in [0,T]}$ and
$(U_t)_{t \in \ccint{0,T}}$ as $\pi_{X}^T$, $\pi_{Y}^T$ and $\pi_{U}^T$
respectively with $T = Kh$.


\newcommand{\minvsp}{0}

We consider the following assumptions: \vspace{\minvsp pt}
\begin{assumption}
\label{asmp:sde_ergo}
For all $\lambda >0$, the SDE  \eqref{eqn:sde} has a unique strong solution denoted by $(X_t)_{t\geq 0}$ for any starting point $x \in \R^d$. %
\vspace{\minvsp pt}
\end{assumption}
\begin{assumption}
\label{asmp:lipschitz}
There exits $L < \infty$ such that
\begin{align}
\| v_t(x) - v_{t'}(x') \| \leq L ( \|x-x' \| + |t-t'|),
\end{align}
where $v_t(x) = v(x,\mu_t)$ and
\begin{align}
\| \hat{v}(x,\mu) - \hat{v}(x',\mu') \| \leq L ( \|x-x' \| + \|\mu-\mu'\|_{\TV}).
\end{align}
\vspace{\minvsp pt}
\end{assumption}
\begin{assumption}
\label{asmp:dissip}
For all $t \geq 0$, $v_t$ is dissipative, i.e. for all $x \in \R^d$,
\begin{align}
\langle x, v_t(x) \rangle \geq m \|x\|^2 -b,
\end{align}
for some $m,b >0$.
\vspace{\minvsp pt}
\end{assumption}
\begin{assumption}
\label{asmp:stochgrad}
The estimator of the drift satisfies the following conditions: \ $\E[\hat{v}_t] = v_t$ for all $t \geq 0$, and for all $t\geq 0$, $x \in \R^d$,
\begin{align}
\E[ \|\hat{v}(x,\mu_t) - v(x,\mu_t) \|^2] \leq 2 \delta(L^2 \|x\|^2 + B^2),
\end{align}
for some $\delta \in (0,1)$. %
\vspace{\minvsp pt}
\end{assumption}
\begin{assumption}
\label{asmp:init_fun}
For all $t \geq 0$: $|\Psi_t(0)| \leq A$ and $\|v_t(0)\| \leq B$,
for $A,B \geq 0$, where $\Psi_t = \int_{\mathbb{S}^{d-1}} \psi_{t}(\ps{\theta}{\cdot}) d \theta$. 
\end{assumption}


 
We start by upper-bounding $\| \muh_{Kh} - \mu_T \|_{\TV}$. 
\begin{lemma}
\label{lem:euler}
Assume that the conditions \Cref{asmp:lipschitz,asmp:stochgrad,asmp:dissip,asmp:init_fun} hold. Then, the following bound holds:
\begin{align}
\| \muh_{Kh} - \mu_{T} \|_{\TV}^2\leq \| \pi^T_{Y} - \pi_{X}^T \|_{\TV}^2 \leq \frac{L^2 K}{4\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) + \frac{C_2 \delta K h}{8\lambda},
\end{align}
where $C_1 \triangleq 12(L^2 C_0 + B^2)+1$, $C_2 \triangleq 2 (L^2 C_0 + B^2)$, $C_0 \triangleq C_e +2  (1 \vee \frac1{m})(b+2B^2 + d \lambda)$, and $C_e$ denotes the entropy of $\mu_0$.
\end{lemma}
\begin{proof}
We use the proof technique presented in \cite{dalalyan2017theoretical,raginsky17a}.  It is easy to verify that for all $k \in \mathbb{N}_+$, we have $Y_{kh} = \hat{X}_k$. 

By Girsanov's theorem to express the Kullback-Leibler (KL) divergence between these two distributions, given as follows:
\begin{align}
\KL (\pi_{X}^T || \pi_{Y}^T) &= \frac1{4 \lambda} \int_0^{Kh} \E[ \|v_t(Y_t) + \tilde{v}_t(Y) \|^2 ]  \> dt \\
&= \frac1{4 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|v_t(Y_t) + \tilde{v}_t(Y) \|^2 ] \> dt \\
&= \frac1{4 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|v_t(Y_t) - \hat{v}_{kh}(Y_{kh}) \|^2 ] \> dt.
\end{align}
By using $v_t(Y_t) - \hat{v}_{kh}(Y_{kh}) = ( v_t(Y_t) - v_{kh}(Y_{kh})) + ( v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh}))$, we obtain
\begin{align}
\nonumber \KL (\pi_{X}^T || \pi_{Y}^T) \leq& \frac1{2 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|v_t(Y_t) - {v}_{kh}(Y_{kh}) \|^2 ] \> dt \\
&+  \frac1{2 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh}) \|^2 ] \> dt \\
\nonumber \leq& \frac{L^2}{\lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \bigl(\E[ \|Y_t - Y_{kh} \|^2 ] + (t-kh)^2 \bigr)  \> dt \\
&+  \frac1{2 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh}) \|^2 ] \> dt . \label{eqn:lem1_proof_interm}
\end{align}
The last inequality is due to the Lipschitz condition \Cref{asmp:lipschitz}.

Now, let us focus on the term $\E[ \|Y_t - Y_{kh} \|^2]$. By using \eqref{eqn:sde_linear}, we obtain:
\begin{align}
Y_t - Y_{kh} = - (t-kh) \hat{v}_{kh}(Y_{kh}) + \sqrt{2 \lambda (t-kh)} Z,
\end{align}
where $Z$ denotes a standard normal random variable. By adding and subtracting the term $-(t-kh) v_{kh}(Y_{kh})$, we have:
\begin{align}
Y_t - Y_{kh} = -(t-kh)v_{kh}(Y_{kh}) + (t-kh)(v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh})) + \sqrt{2 \lambda (t-kh)} Z.
\end{align}
Taking the square and then the expectation of both sides yields:
\begin{align}
\nonumber \E[ \|Y_t - Y_{kh} \|^2] \leq& 3(t-kh)^2 \E[ \|v_{kh}(Y_{kh})\|^2] + 3 (t-kh)^2 \E[\|v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh})\|^2] \\
&+ 6\lambda (t-kh)d.
\end{align}
As a consequence of \Cref{asmp:lipschitz} and \Cref{asmp:init_fun}, we have $\| v_t(x)\| \leq L\|x\|+B$ for all $t \geq 0$, $x\in \R^d$. Combining this inequality with \Cref{asmp:stochgrad}, we obtain:
\begin{align}
\nonumber \E[ \|Y_t - Y_{kh} \|^2] \leq& 6(t-kh)^2 (L^2 \E[ \|Y_{kh}\|^2] + B^2) + 6(t-kh)^2 (L^2 \E[ \|Y_{kh}\|^2] + B^2) \\
&+ 6\lambda (t-kh)d\\
=& 12(t-kh)^2 (L^2 \E[ \|Y_{kh}\|^2] + B^2) + 6\lambda (t-kh)d.
\end{align}
By Lemma 3.2 of \cite{raginsky17a}\footnote{Note that Lemma 3.2 of \cite{raginsky17a} considers the case where the drift is not time- or measure-dependent. However, with \Cref{asmp:dissip} it is easy to show that the same result holds for our case as well.}, we have $\E[ \|Y_{kh}\|^2] \leq C_0 \triangleq C_e +2  (1 \vee \frac1{m})(b+2B^2 + d \lambda)$, where $C_e$ denotes the entropy of $\mu_0$. Using this result in the above equation yields:
\begin{align}
\E[ \|Y_t - Y_{kh} \|^2] \leq& 12(t-kh)^2 (L^2 C_0 + B^2) + 6\lambda (t-kh)d. \label{eqn:lem_bound1}
\end{align}

We now focus on the term $\E[ \|v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh}) \|^2 ]$ in \eqref{eqn:lem1_proof_interm}. Similarly to the previous term, we can upper-bound this term as follows:
\begin{align}
\E[ \|v_{kh}(Y_{kh}) - \hat{v}_{kh}(Y_{kh}) \|^2 ] \leq& 2 \delta(L^2 \E[\|Y_{kh}\|^2] + B^2) \\
\leq& 2 \delta(L^2 C_0 + B^2). \label{eqn:lem_bound2}
\end{align}

By using \eqref{eqn:lem_bound1} and \eqref{eqn:lem_bound2} in \eqref{eqn:lem1_proof_interm}, we obtain:
\begin{align}
\nonumber \KL (\pi_{X}^T || \pi_{Y}^T) \leq& \frac{L^2}{\lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \bigl(12(t-kh)^2 (L^2 C_0 + B^2) + 6\lambda (t-kh)d +(t-kh)^2 \bigr) dt\\
&+  \frac1{2 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} 2 \delta(L^2 C_0 + B^2) \> dt \\
=& \frac{L^2 K}{\lambda} \Bigl( \frac{C_1 h^3}{3} + \frac{6 \lambda d h^2}{2} \Bigr) + \frac{C_2 \delta K h}{2\lambda},
\end{align}
where $C_1 = 12(L^2 C_0 + B^2)+1$ and $C_2 = 2 (L^2 C_0 + B^2)$.

Finally, by using the data processing and Pinsker inequalities, we obtain:
\begin{align}
\| \muh_{Kh} - \mu_{T} \|_{\TV}^2 \leq \| \pi_{X}^T - \pi_{Y}^T \|_{\TV}^2 \leq& \frac1{4} \KL (\pi_{X}^T || \pi_{Y}^T) \\
=& \frac{L^2 K}{4\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) + \frac{C_2 \delta K h}{8\lambda}.
\end{align}
This concludes the proof.
\end{proof}


Now, we bound the term $\| \mub_{Kh} - \muh_{Kh} \|_{\TV}$.
\begin{lemma}
\label{lem:euler2}
Assume that \Cref{asmp:lipschitz} holds. Then the following bound holds:
\begin{align}
\| \pi_{U}^T - \pi_{Y}^T \|_{\TV}^2  \leq \frac{L^2 K h}{16 \lambda}  \|\pi_{X}^T - \pi_{U}^T \|_{\TV}^2 .
\end{align}
\end{lemma}
\begin{proof}
We use that same approach than in Lemma~\ref{lem:euler}. By Girsanov's theorem once again, we have
\begin{align}
\KL (\pi_{Y}^T || \pi_{U}^T) &= \frac1{4 \lambda} \sum_{k=0}^{K-1} \int_{kh}^{(k+1)h} \E[ \|\hat{v}(U_{kh}, \mu_{kh}) - \hat{v}(U_{kh},\mub_{kh}) \|^2 ] \> dt,
\end{align}
where $\pi_U^T$ denotes the distributions of $(U_t)_{t \in [0,T]}$ with $T = Kh$. By using \Cref{asmp:lipschitz}, we have:
\begin{align}
\KL (\pi_{Y}^T || \pi_{U}^T) &\leq \frac{L^2 h}{4 \lambda} \sum_{k=0}^{K-1} \|\mu_{kh} - \mub_{kh} \|_{\TV}^2   \\
&\leq \frac{L^2 K h}{4 \lambda}  \|\pi_{X}^T - \pi_{U}^T \|_{\TV}^2  .
\end{align}
By applying the data processing and Pinsker inequalities, we obtain the desired result.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:euler}}

Here, we precise the statement of Theorem~\ref{thm:euler}.

\begin{thm}
\label{lem:euler3}
Assume that the assumptions in Lemma~\ref{lem:euler} and Lemma~\ref{lem:euler2} hold. Then for $\lambda > \frac{KL^2h}{8}$, the following bound holds:
\begin{align}
\|\mub_{Kh} - \mu_{T} \|_{\TV}^2 &\leq \delta_\lambda \Biggl\{ \frac{L^2 K}{2\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) + \frac{C_2 \delta K h}{4\lambda} \Biggr\},
\end{align}
where $\delta_\lambda = (1 -\frac{KL^2h}{8\lambda})^{-1} $.
\end{thm}
\begin{proof}
We have the following decomposition: (with $T= Kh$)
\begin{align}
\|\pi_{X}^T - \pi_{U}^T \|_{\TV}^2 &\leq 2 \|\pi_{X}^T - \pi_Y^T \|_{\TV}^2 + 2\|\pi_Y^T - \pi_{U}^T \|_{\TV}^2 \\
&\leq  \frac{L^2 K}{2\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) + \frac{C_2 \delta K h}{4\lambda} +  \frac{L^2 K h}{8 \lambda}  \|\pi_X^T - \pi_{U}^T \|_{\TV}^2 \\ 
&\leq \Bigl(1 -\frac{KL^2h}{8\lambda} \Bigr)^{-1} \Biggl\{ \frac{L^2 K}{2\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) + \frac{C_2 \delta K h}{4\lambda} \Biggr\}.
\end{align}
The second line follows from Lemma~\ref{lem:euler} and Lemma~\ref{lem:euler2}. Last line follows from the assumption that $\lambda$ is large enough. This completes the proof.
\end{proof}



\section{Proof of Corollary~\ref{coro:precision}}
\begin{proof}
Considering the bound given in Theorem~\ref{thm:euler}, the choice $h$ implies that
\begin{align}
\frac{\delta_\lambda L^2 K}{2\lambda} \Bigl( \frac{C_1 h^3}{3} + 3 \lambda d h^2 \Bigr) \leq \varepsilon^2. \label{eqn:cor_Th}
\end{align}
This finalizes the proof. 
\end{proof}
















