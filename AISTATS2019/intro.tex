%!TEX root = ./aistats_2019_sketchmcmc.tex

\section{Introduction}


Implicit generative modeling (IGM) \cite{diggle1984monte, mohamed2016learning} has become very popular recently and has proven
successful in various fields; variational auto-encoders (VAE) \cite{kingma2013VAE} and generative adversarial networks (GAN) \cite{goodfellow2014generative} being its two well-known examples. The goal in IGM can be briefly described as learning the
underlying probability measure of a given dataset, denoted as $\nu \in \PS(\Omega)$, where $\PS$ is the space of probability measures on the measurable space $(\Omega,\mca)$, $\Omega \subset \rset^d$ is a domain and $\mca$ is the associated Borel $\sigma$-field.

Given a set of data points $\{y_1 , \dots , y_P \}$ that are assumed to be independent and identically distributed (i.i.d.) samples drawn from $\nu$, the implicit generative framework models the data points as the output of a measurable map, i.e.\ $y = T(x)$, with $T: \Omega_\mu \mapsto \Omega$. Here, the inputs $x$ are generated from a known and easy to sample source measure $\mu$ on $\Omega_\mu$ (e.g.\ Gaussian or uniform measures), and the outputs $T(x)$ should match the unknown target measure $\nu$ on $\Omega$.




Learning generative networks has witnessed several groundbreaking contributions in the recent years. Motivated by this fact, there has been an interest in illuminating the theoretical foundations of VAEs and GANs \cite{bousquet2017optimal,liu2017approximation}.
%
It has been shown that these implicit models have close connections with the theory of Optimal Transport (OT) \cite{villani2008optimal}.
%
As it turns out, OT brings new light on the generative modeling problem: there have been several extensions of VAEs \cite{tolstikhin2017wasserstein,kolouri2018sliced} and GANs \cite{arjovsky2017wasserstein,gulrajani2017improved,guo2017relaxed,lei2017geometric}, which exploit the links betwen OT and IGM.


OT studies whether it is possible to transform samples from a source distribution $\mu$ to a target distribution $\nu$. From this perspective, an ideal generative model is simply a transport map from $\mu$ to $\nu$.
This can be written by using some `push-forward operators': we seek a mapping $T$ that `pushes $\mu$ onto $\nu$', and is formally defined as $ \nu(A) =  \mu(T^{-1}(A)) $ for all Borel sets $A \subset \mca$. If this relation holds, we denote the push-forward operator $T_\#$, such that $T_\# \mu = \nu$. Provided mild conditions on these distributions hold (notably $\mu$ is non-atomic \cite{villani2008optimal}), existence of such a transport map is guaranteed; however, it remains a challenge to construct it in practice.

One common point between VAE and GAN is to adopt an approximate strategy and consider transport maps that belong to a parametric family $T_{\phi}$ with $\phi \in \Phi$. Then, they aim at finding the best parameter $\phi^\star$ that would give $T_{\phi^\star \#}\mu \approx \nu$. This is typically achieved by attempting to minimize the following optimization problem:
% \begin{align}
$\phi^\star = \argmin_{\phi \in \Phi} \W(T_{\phi \#}\mu, \nu)$,
% \end{align}
where $\W$ denotes the Wasserstein distance that will be properly defined in Section~\ref{sec:techbg}. It has been shown that \cite{genevay2017gan} OT-based GANs \cite{arjovsky2017wasserstein} and VAEs \cite{tolstikhin2017wasserstein} both use this formulation with different parameterizations and different equivalent definitions of $\W$. However, their resulting algorithms still lack theoretical understanding.

% which will have theoretical performance guarantees.

In this study, we follow a completely different approach for IGM, where we aim at developing an algorithm with explicit theoretical guarantees for estimating a transport map between source $\mu$ and target $\nu$. The generated transport map  will be \textit{nonparametric} (in the sense that it does not belong to some family of functions, like a neural network), and it will be iteratively augmented: always increasing the quality of the fit along iterations. Formally, we take $T_t$ as the constructed transport map at time $t \in [0,\infty)$, and define $\mu_t=T_t \# \mu$ as the corresponding output distribution. Our objective is to build the maps so that $\mu_t$ will converge to the solution of a functional optimization problem, defined through a gradient flow in the Wasserstein space. Informally, we will consider a gradient flow that has the following form:
\begin{align}
\partial_t \mu_t = - \nabla_{\W} \Bigl\{ \mathrm{Cost}(\mu_t, \nu) + \mathrm{Reg}(\mu_t)\Bigr\} \, , \>\> \mu_0 = \mu,\label{eqn:gradflow}
\end{align}
where the functional $\mathrm{Cost}$ computes a discrepancy between $\mu_t$ and $\nu$, $\mathrm{Reg}$ denotes a regularization functional, and $\nabla_{\W}$ denotes a notion of gradient with respect to a probability measure in the $\W$ metric for probability measures\footnote{This gradient flow is similar to the usual Euclidean gradient flows, i.e.\ $\partial_t x_t = - \nabla (f(x_t) + r(x_t))$, where $f$ is typically the data-dependent cost function and $r$ is a regularization term. The (explicit) Euler discretization of this flow results in the well-known gradient descent algorithm for solving $\min_x (f(x)+r(x))$.}. If this flow can be simulated, one would hope for $\mu_t=(T_t)_{\#}\mu$ to converge to the minimum of the functional optimization problem: $\min_\mu ( \mathrm{Cost}(\mu, \nu) + \mathrm{Reg}(\mu)$) \cite{ambrosio2008gradient,santambrogio2017euclidean}.


% In this study,
We construct a gradient flow where we choose the $\mathrm{Cost}$ functional as the \textit{sliced Wasserstein distance} ($\SW$) \cite{bonneel2015sliced} and the $\mathrm{Reg}$ functional as the negative entropy. The $\SW$ distance is equivalent to the $\W$ distance \cite{bonnotte2013unidimensional} and has important computational implications since it can be expressed as an average of (one-dimensional) projected optimal transportation costs whose analytical expressions are available.

We first show that, with the choice of $\SW$ and the negative-entropy functionals as the overall objective, we obtain a valid gradient flow that has a solution path $(\mu_t)_t$, and the probability density functions of this path solve a particular partial differential equation, which has close connections with stochastic differential equations. Even though gradient flows in Wasserstein spaces cannot be solved in general, by exploiting this connection, we are able to develop a practical algorithm that provides approximate solutions to the gradient flow and is algorithmically similar to stochastic gradient Markov Chain Monte Carlo (MCMC) methods \cite{WelTeh2011a,raginsky17a}. We provide finite-time error guarantees for the proposed algorithm and show explicit dependence of the error to the algorithm parameters.



To the best of our knowledge, the proposed algorithm is the first nonparametric IGM algorithm that has explicit theoretical guarantees. In addition to its nice theoretical properties, the proposed algorithm has also significant practical importance: it has low computationally requirements and can be easily run on an everyday laptop CPU. On the other hand, it might have interesting implications in other domains, such as differential-privacy \cite{dwork2014algorithmic}, since it only requires random projections of the data, rather than the data itself. Our experiments on both synthetic and real datasets support our theory and illustrate the advantages of the algorithm in several scenarios.







%%% Local Variables:
%%% mode: latex
%%% TeX-master: "aistats_2019_sketchmcmc"
%%% End:
