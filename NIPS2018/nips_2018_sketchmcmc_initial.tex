\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\newcommand{\W}{{\cal W}_2}
\newcommand{\F}{{\cal F}}
\newcommand{\He}{{\cal H}}
\newcommand{\SW}{{\cal S}{\cal W}_2}

\DeclareMathOperator*{\argmin}{arg\min}



\title{Sliced-Wasserstein Flows: Learning Generative Models via Single Data Pass with Guarantees}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

\end{abstract}

\section{Constructing an entropy-regularized $\SW$ gradient flow}

Bonnotte \cite{bonnotte2013unidimensional} considers the IDT algorithm \cite{pitie2005n} and develops a continuity equation, given as follows:
\begin{align}
\partial_t \rho_t - \nabla \cdot (\rho_t v_t) = 0,
\end{align}
where $\rho_t$ is the density of $\mu_t$ and
\begin{align}
v_t(x) \triangleq \int_{\mathbb{S}^{d-1}} \psi'_{t,\theta}(\langle \theta, x \rangle) \theta \> d\theta. \label{eqn:idt_v}
\end{align}
Here, $\psi_{t,\theta}$ denotes the Kantorovich potential between $\theta^*_{\#}\mu_t$ and $\theta^*_{\#}\nu$. In fact, one can show that, this is nothing but a gradient flow in the Wasserstein spaces, given as follows:
\begin{align}
\partial_t \rho = - \nabla_{\W} \F(\rho) \label{eqn:gradflow}
\end{align}
where $\nabla_{\W}$ denotes a notion of gradient in the $\W$ metric and the functional $\F$ is chosen as the squared sliced-Wasserstein distance between $\mu$ and $\nu$:
\begin{align}
\F(\rho) \triangleq \frac1{2} \SW^2(\rho, \pi)
\end{align}
where $\pi$ denotes the density of $\nu$. Here we abused the notation by defining $\SW$ on the densities instead of the measures; we implicitly assume that both measures $\mu$ and $\nu$ are dominated by the Lebesgue measure. This gradient flow basically constructs a path $(\rho_t)_{t\geq 0}$ that minimizes $\F$ as $t$ increases. In other words, the goal in construction such a flow is to solve the following problem:
\begin{align}
\rho^\star = \argmin_\rho \F(\rho).
\end{align}
Since we obviously have $\rho^\star = \pi$, this gradient flow will start from $\rho_0$ and bring it closer to $\pi$ as $t$ evolves.

By following \cite{santambrogio2017euclidean}, the gradient given in \eqref{eqn:gradflow} can be written as follows:
\begin{align}
\nabla_{\W} \F(\rho) = -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho}(\rho) \bigr) \Bigr), \label{eqn:gradw2}
\end{align}
where $\frac{\delta \F}{\delta \rho}(\rho)$ denotes the first variation of $\F$.

By this definition, $\mu_t$ directly goes to $\nu$. However, in practical settings, we will have finitely many samples from $\mu_0$ and $\nu$, therefore this scheme will somehow `overfit' to the data distribution. Therefore what we propose is to somehow `regularize' the gradient flow by introducing an entropy term to the minimization process. In particular, we modify the gradient flow given in \eqref{eqn:gradflow} as follows:
\begin{align}
\partial_t \rho = - \nabla_{\W} \F_\lambda(\rho),
\end{align}
where
\begin{align}
\F_\lambda(\rho) \triangleq \F(\rho) + \lambda \He(\rho).
\end{align}
Here, $\He(\rho) \triangleq \int (h \circ \rho) (x) dx $ denotes the negative entropy of $\rho$ with $h(t) = t \log t$. This regularization somewhat corresponds to assuming a Gaussian prior on the density $\rho$: when $\lambda$ goes to infinity, the optimal $\rho$ that minimizes $\F_\lambda$ will be a Gaussian density since the Gaussian densities have the maximum entropy.

This time the optimization problem is modified:
\begin{align}
\min_\rho \F_\lambda(\rho),
\end{align}
in which $\pi$ is no longer an optimizer. The idea in this new gradient flow formulation is to take $\rho_t$ as close as possible to $\pi$, while trying to keep its entropy at a certain level, so that it would be expressive for generative modeling purposes.

We need to compute the gradient of $\F_\lambda$ by using the definition given in \eqref{eqn:gradw2}. We first start by computing the first variation of the functional:
\begin{align}
\frac{\delta \F_\lambda}{\delta \rho} (\rho ) &= \frac{\delta \F}{\delta \rho} (\rho ) + \frac{\delta \He}{\delta \rho} (\rho ) \\
&= \frac{\delta \F}{\delta \rho} + \lambda  (\log \rho +1 ) .
\end{align}
When we use this identity in \eqref{eqn:gradw2}:
\begin{align}
\nabla_{\W} \F_\lambda(\rho) &= -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F_\lambda}{\delta \rho}(\rho) \bigr) \Bigr) \\
&= -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho} +  \lambda  (\log \rho +1 ) \bigr) \Bigr) \\
&= -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho} \bigr) \Bigr) -  \lambda \nabla \cdot \Bigl( \rho \nabla \bigl( \log \rho +1  \bigr) \Bigr)
\end{align}
By using $\nabla ( \log \rho +1  ) = \nabla \log \rho = \frac{\nabla \rho}{\rho} $, we have:
\begin{align}
\nabla_{\W} \F_\lambda(\rho) &= \nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho} \bigr) \Bigr) -  \lambda \nabla \cdot \Bigl( \nabla \rho \Bigr) \\
&= \nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho} \bigr) \Bigr) -  \lambda \Delta \rho,
\end{align}
where $\Delta$ denotes the Laplacian operator.

Since we already know the definition of $\nabla \bigl( \frac{\delta \F}{\delta \rho} \bigr)$ from \eqref{eqn:idt_v}, we can now construct the modified gradient flow as follows:
\begin{align}
\partial_t \rho &= - \nabla_{\W} \F_\lambda(\rho) \\
&=  \nabla \cdot (\rho \> v_t) + \lambda \Delta \rho. \label{eqn:gradflow_reg}
\end{align}

\section{Connecting with stochastic differential equations}

We now consider the modified flow given in \eqref{eqn:gradflow_reg}. We can observe that, this equation is the Fokker-Planck equation associated with the following stochastic differential equation (SDE):
\begin{align}
d X_t = - v_t(X_t) dt + \sqrt{2 \lambda } d W_t,
\end{align}
where $W_t$ denotes the standard Brownian motion. In practice we can simulate this SDE by using the Euler-Maruyama scheme:
\begin{align}
X_{n+1} = X_n - h v_n(X_n) + \sqrt{2 \lambda h} Z_{n+1},
\end{align}
where $\{Z_n\}_{n}$ denotes a series of standard Gaussian random variables and $h$ denotes the step-size. In practical applications, it will not be possible to exactly simulate $v_n$, therefore we will need to develop an unbiased estimator of $v_n$, such that $\mathbb{E}[\hat{v}_n (x)] = v_n(x)$ for all $n$ and $x$. After that we might hope to have some error bounds on our estimation.



\section{Open questions}

\begin{enumerate}
\item We assume that the flow given in Bonnotte converges to $\nu$. Can we say/prove something about this?
\\Do you think there is some restrictive assumption to put on $\nu$ that would make it clear? Something less limiting than the Gaussian assumption, that's kind of useless. What about your intuition of overfitting? Let's say that actually, we do not observe $\nu$, but rather some $\hat{\nu}$ that is atomic, and just a sum of diracs over the observations, maybe convergence to $\hat{\nu}$ is obtained through Bonnotte's scheme? The problem is that actually, this solution is probably not our true objective, and these stuff would kind of justify your idea of justifiying our regularisation as avoiding overfitting.
\item We need to develop an unbiased (or maybe biased) estimator for $v_n$. We also need to think about the `single-data-pass' aspect? Can we still do a single data pass in this scheme?
\\ I understand this means we approximate the sum over $\theta$ as a finite sum, right? It's still a bit unclear to me, but in the 1D case, $\psi'_{t,\theta}(\langle \theta, x \rangle)$ is precisely given by the transport map (for which we have the analytical expression), right? In this case, I think I will implement this approximation by picking a different set of $\theta$ each time, just like they do in IDT.
\\Concerning the single data-pass aspect, maybe I'm wrong but I think it's not changed: all we need for building the transport map, whatever the current $\rho_t$, is the distribution $\theta^*_{\#}\nu$ (that does not depend on $\rho_t$). Correct me if I'm wrong, but in practice, it looks to me we are simply adding a Gaussian noise term to the solution of the IDT, right?
\item When we use the entropy regularization, the flow will no longer converge to $\nu$ (assuming the first flow converges to $\nu$). Let's say it converges to $\nu_\lambda$. Can we show a bound between $\nu$ and $\nu_\lambda$?
\\I think some answers will be found in~\cite{cuturi2013sinkhorn}
\item Let's say we developed an estimator for $v_n$ (or more generally $v_t$). Can we show error bounds? I am sure there are related studies to this. I will check the relevant literature. I am guessing that we might need to assume some sort of regularity in $v_t$ in terms of $t$.
\item This scheme reminds me the normalizing flows \cite{rezende2015variational}, continuous-time flows \cite{chen2018cont} and Stein Variational Descent \cite{liu2016stein,liu2017stein}. We need to understand the differences/similarities.
\end{enumerate}


\bibliography{./references.bib}
\bibliographystyle{unsrt}



\end{document}
