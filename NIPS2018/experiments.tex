%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Experiments}

\begin{figure}
\subfigure[]{
	\includegraphics[width=0.65\columnwidth]{figures/gmm_iter.pdf}
	\label{fig:toy_example}
} \hfill
\subfigure[]{
\includegraphics[width=0.255\columnwidth]{figures/gmm_reg.pdf}
\label{fig:lambda_supp}
 }
 \vspace{-\baselineskip}
\caption{a) \textbf{Left:} Distribution of particles (contour plots) during the estimation (top) and prediction (bottom) stages. \textbf{Right:} (top) Close-up of some generated particles in red superimposed with data points in black. (bottom) Target distribution. b) Influence of the regularization parameter~$\lambda$. }
\end{figure}

%I think the real procedure is not helping anything computationally and making the presentation a bit confusing.
In this section, we evaluate the SWF algorithm on both synthetic and real data settings. In all cases, the initial distribution $\mu_0$ is selected as the standard Gaussian distribution on $\R^d$, we take $Q=100$ quantiles, which proved sufficient to approximate these functions faithfully, and we have observed that $N=3000$ particles are sufficient.

% \footnote{}


% In this section, we evaluate the SWF algorithm on both synthetic and real data settings. In all cases, the $N$ initial particles $\bar{X}^i_0\in\mathbb{R}^d$ are obtained by first drawing i.i.d.\ standard Gaussian $\bar{Z}^i\in\mathbb{R}^m$ with $m\ll d$. Then we generate random matrix $A$ of size $d\times m$, and finally set $\bar{X}^i_0=A\bar{Z}^i$. This results in $\mu_0=\mathcal{N}(0,AA^\top)$. %This procedure allows us to use the with an arbitrary input dimension.

% \subsection{Toy example: multivariate Gaussian Mixture Model}
% \label{sub:toy_example}

\begin{wrapfigure}{R}{0.40\textwidth}
\vspace{-10pt}
\begin{centering}
\includegraphics[width=0.40\columnwidth]{figures/SW2_cost-crop.pdf}
\par\end{centering}
\caption{Approximately computed $\SW$ between the output $\bar{\mu}_{k}^{N}$ and data distribution $\nu$ in the GMM model for different data dimensions $d$.
\label{fig:toy_sw}}
\vspace{-10pt}
\end{wrapfigure}
\textbf{Gaussian Mixture Model: }
We perform the first set of experiments on synthetic data where we consider a standard Gaussian mixture model (GMM).
%
We set the number of the mixture components to $20$ and for each component we randomly draw the weight, covariance matrices and centroids. We make sure that the centroids are sufficiently distant from each other in order to make the problem more challenging. Given the model parameters, we generate $P=50000$ data samples in each experiment.


In our first experiment, we set $d=2$ for visualization purposes and illustrate the general behavior of the algorithm. Figure~\ref{fig:toy_example} shows the evolution of the particles through the iterations. Here, we set $N_\theta=30$, $h=1$, and $\lambda=10^{-4}$.
%
We observe that the empirical distribution of the particles converges rapidly to the target distribution. Furthermore, we can see that the QF, $F^{-1}_{\theta^*_\#\bar{\mu}_{kh}^{N}}$ that is computed with one set of particles (so-called the \textit{estimation} stage) can be perfectly re-used for new unseen particles in a subsequent \textit{prediction} stage. In both cases, we observe two remarkable outcomes: (i) Even when some modes are isolated from the others, SWF is able to capture them successfully and we never observe a mode collapse. This is due to the OT nature of the procedure. (ii) The generated particles do not collapse on the data points, thanks to the entropy regularization.

In our second experiment, we investigate the effect of the level of the regularization. We use the same setting as the previous experiment, whereas we differ the value of $\lambda$ and run the algorithm for sufficiently many iterations. As we can observe from Figure~\ref{fig:lambda_supp}, the distribution of the particles becomes more spread with increasing $\lambda$. This is due to the increment of the entropy, as expected.

% There, the target distribution is displayed on the rightmost panel, as obtained through a contour plot of the data samples. On the left panel, we see the output distribution of the SWF at different iterations.


We also illustrate the behavior of the algorithm for varying dimensionality $d$. Since visualizing the results becomes non-trivial for large $d$, in this experiment we directly monitor the (approximately computed) $\SW$ distance between the distribution of the particles and the data distribution. Even though minimizing this distance is not the real objective of our method, arguably, it is still a good proxy for understanding the convergence behavior.
%
Figure~\ref{fig:toy_sw} illustrates the results. We observe that, for all choices of $d$, we see a steady decrease in the cost for all runs, which is in line with our theory. We also observe that the magnitude of $\SW$ decreases as $d$ increases. This outcome can be explained by the fact that $\SW \leq \W \leq \SW^{{\cal O}(1/d)}$ (cf.\ \cite{bonnotte2013unidimensional,santambrogio2015optimal}).
%
<<<<<<< HEAD
Since we used the same $\lambda$ for all $d$, the entropy term becomes relatively stronger in the case of smaller $d$, which results in larger $\SW$. This result suggests that, in order to achieve the same level of regularization, we should use larger $\lambda$ as the dimensionality increases.
=======
% Since we used the same $\lambda$ for all $d$, the entropy term becomes relatively stronger in the case of smaller $d$, which results in larger $\SW$. This result suggests that, in order to achieve the same level of regularization, we should use larger $\lambda$ as the dimensionality increases.
>>>>>>> 102e2b1c63d268694a3af2161774bc41d9f238ca




% Our next experiment aims to assess whether SWF achieves its objective of decreasing the sliced Wasserstein distance between the output $\bar{\mu}_{k}^{N}$ and the target $\nu$. For each run, this is done by computing \label{eqn:sw} using an empirical average over random directions $\theta\in\Sp^{d-1}$ and the analytical expression \eqref{eq:W1D} for the scalar Wasserstein distance.







% \subsection{Experiments on real data}
% \label{sub:real_data}

\textbf{Experiments on real data: }
%
In a second set of experiments, we test the SWF algorithm on two real datasets. (i) The traditional MNIST dataset that contains 70K binary images corresponding to different digits (of size $28 \times 28$, i.e.\ $d = 784$). (ii) The recently proposed FashionMNIST dataset \cite{xiao2017fashion}, that contains $50000$ gray-scale images. All images were interpolated as $64\times 64$, yielding $d=4096$. This dataset is advocated as more challenging than MNIST.

Our goal in these experiments is to capture the structure of the data distribution such that the particles that are generated by the algorithm will be samples from this unknown data distribution. In these experiments, we set $\lambda=10^{-6}$ and $N_\theta=200$. We will present visual results for qualitative inspection. More results with higher resolution are given in the supplementary document.

Figures~\ref{fig:mnist} and \ref{fig:fashionmnist}, show that SWF is able to generate samples from the dataset in a few thousand iterations. We can observe that, the generated samples for the MNIST dataset are considerably accurate. For the FashionMNIST dataset, the samples capture the prominent features of the training samples; however, some of the fine-grained textures are not captured by the method. By considering that SWF only requires the projections of the data points, in a way, all these samples are generated without seeing the actual dataset.


Another important advantage of SWF is its low computational requirements. The whole experiment on the FashionMNIST requires around $1$ hour of computational time on the CPU of a standard laptop computer, to be compared with the significant resource requirements of the current IGM methods.








\begin{figure}[t]
\centering
\subfigure[MNIST]{
\includegraphics[width=0.41\columnwidth]{figures/mnist.pdf}
\label{fig:mnist}
}\hfill
\subfigure[Fashion MNIST]{
\includegraphics[width=0.55\columnwidth]{figures/fmnist2.pdf}
\label{fig:fashionmnist}
}
\vspace{-9pt}
\caption{The evolution of the particles through $15000$ iterations on different datasets.}
% \vspace{-10pt}
\end{figure}

% \begin{figure}
% \begin{centering}
%   \setlength\tabcolsep{1pt}

% \begin{tabular}{cccc|c}
% $k=200$ & $k=2000$ & $k=10000$ & $k=40000$ & target\tabularnewline
% \includegraphics[height=6cm]{figures/FashionMNIST/image_200} & \includegraphics[height=6cm]{figures/FashionMNIST/image_2000} & \includegraphics[height=6cm]{figures/FashionMNIST/image_10000} & \includegraphics[height=6cm]{figures/FashionMNIST/image_38400} & \includegraphics[height=6cm]{figures/FashionMNIST/dataset_example}\tabularnewline
% \end{tabular}
% \par\end{centering}
% \caption{Applying SWF on the Fashion MNIST dataset.\label{fig:fashionmnist}}
% \end{figure}



% \end{itemize}



% \begin{figure}
% \begin{subfigure}[t]{0.5\textwidth}
% \begin{centering}
% \setlength\tabcolsep{1pt}
% \begin{tabular}{ccc|c}
% $k=1$ & $k=7$ & $k=70$ & target\tabularnewline
% \includegraphics[width=2cm]{figures/toy_save/output_dist_k=1-crop.pdf} & \includegraphics[width=2cm]{figures/toy_save/output_dist_k=20-crop.pdf} & \includegraphics[width=2cm]{figures/toy_save/output_dist_k=70-crop.pdf} &  \includegraphics[width=2cm]{figures/scatter_output_particles_k=70-crop.pdf}\tabularnewline
% \includegraphics[width=2cm]{figures/toy_load/output_dist_k=1-crop.pdf} & \includegraphics[width=2cm]{figures/toy_load/output_dist_k=20-crop.pdf} & \includegraphics[width=2cm]{figures/toy_load/output_dist_k=70-crop.pdf} &  \includegraphics[width=2cm]{figures/target-crop.pdf}\tabularnewline
% \end{tabular}
% % \par
% \end{centering}
% \caption{Toy example.}
% \label{fig:toy_example}
% \end{subfigure}
% \end{figure}

% \begin{figure}
% \begin{centering}
% \setlength\tabcolsep{1pt}
% \begin{tabular}{cccccc}
% $\lambda=0$ & $\lambda=0.1$ & $\lambda=0.2$ & $\lambda=0.3$ & $\lambda=0.5$ & $\lambda=1$\tabularnewline
% \includegraphics[width=2cm]{figures/supplementary/regularization/r0_output_dist_k=70-crop.pdf} & \includegraphics[width=2cm]{figures/supplementary/regularization/r01_output_dist_k=70-crop.pdf} & \includegraphics[width=2cm]{figures/supplementary/regularization/r02_output_dist_k=70-crop.pdf} &  \includegraphics[width=2cm]{figures/supplementary/regularization/r03_output_dist_k=70-crop.pdf}&  \includegraphics[width=2cm]{figures/supplementary/regularization/r05_output_dist_k=70-crop.pdf} &  \includegraphics[width=2cm]{figures/supplementary/regularization/r1_output_dist_k=70-crop.pdf}
% \end{tabular}
% \par
% \end{centering}
% \caption{Influence of the regularization parameter $\lambda$. The higher $\lambda$, the more entropic the output distribution.\label{fig:lambda_supp}}
% \end{figure}








% \newcommand{\ww}{0.2}


% \begin{minipage}{\linewidth}
% \begin{minipage}{0.4\textwidth}
% \begin{figure}[H]
% \centering
% \begin{tabular}{ccc|c}
% $k=1$ & $k=7$ & $k=70$ & target\tabularnewline
% \includegraphics[width=0.1\columnwidth]{figures/toy_save/output_dist_k=1-crop.pdf} & \includegraphics[width=0.1\columnwidth]{figures/toy_save/output_dist_k=20-crop.pdf} & \includegraphics[width=0.1\columnwidth]{figures/toy_save/output_dist_k=70-crop.pdf} &  \includegraphics[width=0.1\columnwidth]{figures/scatter_output_particles_k=70-crop.pdf}\tabularnewline
% \includegraphics[width=0.1\columnwidth]{figures/toy_load/output_dist_k=1-crop.pdf} & \includegraphics[width=0.1\columnwidth]{figures/toy_load/output_dist_k=20-crop.pdf} & \includegraphics[width=0.1\columnwidth]{figures/toy_load/output_dist_k=70-crop.pdf} &  \includegraphics[width=0.1\columnwidth]{figures/target-crop.pdf}\tabularnewline
% \end{tabular}
% \par
% \caption{Toy example: training data $y_i$ consists in $50 000$ samples drawn from a Gaussian Mixture Model with $20$ components of random weights, covariances and locations. \textbf{Left:} Output distribution for learning (top) and testing (bottom) particles. \textbf{Right:} (top) Close-up of some generated samples in red superimposed with learning points in black. (bottom) Target distribution. $N_\theta=30$, $N=3000$, $h=1$, $\lambda=10^{-4}$.
% \label{fig:toy_example}}
% \end{figure}
% \end{minipage}
% %
% \begin{minipage}{0.49\textwidth}
% \begin{figure}[H]
% \centering
% \begin{tabular}{cccc|c}
% $k=2K$ & $k=20K$ & $k=10K$ & $k=40K$ & target\tabularnewline
% \includegraphics[width=\ww\columnwidth]{figures/FashionMNIST/image_200} & \includegraphics[width=\ww\columnwidth]{figures/FashionMNIST/image_2000} & \includegraphics[width=\ww\columnwidth]{figures/FashionMNIST/image_10000} & \includegraphics[width=\ww\columnwidth]{figures/FashionMNIST/image_38400} & \includegraphics[width=\ww\columnwidth]{figures/FashionMNIST/dataset_example}\tabularnewline
% \end{tabular}
% \par
% \caption{Applying SWF on the Fashion MNIST dataset.
% \label{fig:fashionmnist}}
% \end{figure}
% \end{minipage}
% \end{minipage}
