%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Introduction}

% \begin{itemize}
% \item Short intro to implicit generative models
% \item Connection with optimal transport \cite{genevay2017gan}, several other papers
% \item Information about sliced-Wasserstein distance and usage in generative modeling \cite{bonnotte2013unidimensional,kolouri2018sliced,wu2017generative}
% \item Contributions of the current paper
% \begin{itemize}
% \item Development of a novel gradient flow for generative modeling purpose
% \item Establish the connections with SDEs
% \item Develop a practical way to simulate the SDE
% \item Establish theoretical guarantees
% \item Experimental validation
% \end{itemize}
% \end{itemize}



% implicit modeling

Implicit generative modeling (IGM) \cite{diggle1984monte, mohamed2016learning} has become very popular recently and has proven
successful in various fields. While the Variational auto-encoders (VAE) \cite{kingma2013VAE} and generative adversarial networks (GAN) \cite{goodfellow2014generative} being the two well-known examples, the goal in these approaches can be briefly described as learning the
underlying probability measure of a given dataset, denoted as $\nu \in \PS(\Omega)$, where $\PS$ is the space of probability measures on the measurable space $(\Omega,\mcf)$, $\Omega \subset \rset^d$ is a (usually compact) domain of $\rset^d$ and $\mcf$ is the associated Borel $\sigma$-field. 

Given a set of data points $\{y_1 , \dots , y_P \}$ that are assumed to be independent and identically distributed (i.i.d.) samples drawn from $\nu$, the implicit generative framework models the data points as the output of a measurable map, i.e.\ $y = T(x)$, with $T: \Omega_\mu \mapsto \Omega$. Here, the inputs $x$ are generated from a known and easy to sample source measure $\mu$ on $\Omega_\mu$ (e.g.\ Gaussian or uniform measures), and the outputs $T(x)$ should match the unknown target measure $\nu$ on $\Omega$. 

% While $\nu$ is not known in closed form, we have accessed to $N$ elements from a learning database $D = \{y_1 , \dots , y_N \}$ which are assumed independently sampled from $\nu$. It is common in applications to have a very large amount of millions or billions of such samples.


% VAE and GAN quickly
% In that context, learning generative networks witnessed several groundbreaking contributions in the recent years. First, variational auto-encoders (VAE \cite{kingma2013VAE}) were proposed that adopt an encoder-decoder strategy, for which the output of the encoder ought to match some prescribed distribution $\mu$, while the combination of encoder and decoder should be as close as possible to identity when evaluated on the training data. The hope of such an approach is to easily generate samples from $\nu$ by applying the decoder to new samples from $\mu$, and not only to encoded training points.%maybe a bit more of references ?

% While VAE showed great potential in generative modeling, they are known to yield blurry samples and yet another important contribution came with Generative Adversarial Networks (GAN, \cite{goodfellow2014generative,salimans2016improved,eghbal2017probabilistic}), which take generative learning as an adversarial game. On the one hand, a discriminative system is trained to detect generated samples from those of the dataset. On the other hand, the generative network is trained so as to minimize the performance of the classifier. Optimization then alternates between both.

% now only talk about OT

% While GAN were both found effective, although delicate to train, 

Learning generative networks has witnessed several groundbreaking contributions in the recent years. Motivated by this fact, there has been an interest in illuminating the theoretical foundations of VAEs and GANs \cite{bousquet2017optimal,liu2017approximation}. 
%
It has been shown that these implicit models have close connections to the theory of Optimal Transport (OT) \cite{villani2008optimal}, 
%
As it turns out, OT brings new light on the generative modeling problem. There have been several extensions of VAEs \cite{tolstikhin2017wasserstein,kolouri2018sliced} and GANs \cite{arjovsky2017wasserstein,gulrajani2017improved,guo2017relaxed,lei2017geometric}, which exploit the links with OT and IGM. 
% Besides, it has been shown that Wasserstein GANs \cite{arjovsky2017wasserstein} and Wasserstein VAEs \cite{tolstikhin2017wasserstein} tackle the same problem in a dual way \cite{bousquet2017optimal,genevay2017gan}. 

% in which such distances between distributions arise naturally.

OT studies whether it is possible to transform samples from a source distribution $\mu$ to a target distribution $\nu$. From this perspective, an ideal generative model is simply a transport map from $\mu$ to $\nu$.  
This can be explained using some `push-forward operators': we seek a mapping $T$ that `pushes $\mu$ onto $\nu$', and is formally defined as $\int_A \nu(dx) = \int_{T^{-1}(A)} \mu(dx) $ for all Borel set $A \subset \mcf$. If this relation holds, we denote the push-forward operator $T_\#$, such that $T_\# \mu = \nu$. Provided very mild conditions on these distributions are respected (notably that $\mu$ is non-atomic \cite{villani2008optimal}), existence and uniqueness of such a transport map is guaranteed; however, it remains a challenge to construct it in practice.

One common point between VAE and GAN is to adopt an approximate strategy and consider transport maps that belong to a parametric family $T_{\phi}$ with $\phi \in \Phi$. Then, they aim at finding the best parameter $\phi^\star$ that would give $T_{\phi^\star \#}\mu \approx \nu$. This is typically achieved by attempting to minimize the following optimization problem:
% \begin{align}
$\phi^\star = \argmin_{\phi \in \Phi} \W(T_{\phi \#}\mu, \nu)$,
% \end{align}
where $\W$ denotes the Wasserstein distance that will be properly defined in Section~\ref{sec:techbg}.  \cite{genevay2017gan} showed that Wasserstein GANs \cite{arjovsky2017wasserstein} and Wasserstein VAEs \cite{tolstikhin2017wasserstein} both use this formulation with different parameterizations and different equivalent definitions of the Wasserstein distance. %, additionally showing that GAN actually learn parameterized Kantorovitch potentials, which are derivatives of transport maps and much less well-behaved. This is advocated as possibly explaining the difficulty of GAN training.

In this study, we follow a completely different approach for IGM and we seek to estimate a transport map between source $\mu$ and target $\nu$ that is \textit{nonparametric}, but rather iteratively augmented, always increasing the quality of the fit along iterations. Formally, we take $T_t$ as the constructed transport map at time $t$, and define $\mu_t=T_t \# \mu$ as the corresponding output distribution. Our objective is to build the maps so that $\mu_t$ will converge to the solution of a functional optimization problem, defined through a gradient flow in the Wasserstein space $(\PS(\Omega),\W)$. Informally, we will consider a gradient flow that have the following form:
\begin{align}
\partial_t \mu_t = - \nabla_{\W} \Bigl\{ \mathrm{Cost}(\mu_t, \nu) + \mathrm{Reg}(\mu_t)\Bigr\} \, , \qquad \mu_0 = \mu,\label{eqn:gradflow}
\end{align}
where the functional $\mathrm{Cost}$ computes a discrepancy between $\mu_t$ and $\nu$, $\mathrm{Reg}$ denotes a regularization functional, and $\nabla_{\W}$ denotes a notion of gradient with respect to a probability measure in the $\W$ metric for probability measures\footnote{This gradient flow is similar to the usual Euclidean gradient flows, i.e.\ $\partial_t x_t = - \nabla (f(x_t) + r(x_t))$, where $f$ is typically the data-dependent cost function and $r$ is a regularization term. The (explicit) Euler discretization of this flow results in the well-known gradient descent algorithm for solving $\min_x (f(x)+r(x))$.}. If this flow can be simulated, one would hope for $\mu_t=T_t\#\mu$ to converge to the minimum of the functional optimization problem: $\min_\mu ( \mathrm{Cost}(\mu, \nu) + \mathrm{Reg}(\mu)$).


In general the gradient flows in Wasserstein spaces cannot be either solved or simulated perfectly, independent of choice of the $\mathrm{Cost}$ functional. In this study, we construct a gradient flow where we choose the $\mathrm{Cost}$ functional as the \textit{sliced Wasserstein distance} ($\SW$) and the $\mathrm{Reg}$ functional as the negative entropy. The $\SW$ distance is equivalent to the $\W$ distance \cite{bonnotte2013unidimensional} and has important computational implications since it can be expressed as an average of (one-dimensional) projected optimal transportation costs whose analytical expressions are available. 

We first show that, with the choice of the $\SW$ and the negative-entropy functionals, we obtain a valid gradient flow that has a solution path $(\mu_t)_t$, and the probability density functions of this path solve a particular partial differential equation, which has close connections with stochastic differential equations. By exploiting this connection, we develop a practical algorithm that is reminiscent of stochastic gradient Markov Chain Monte Carlo (MCMC) methods \cite{WelTeh2011a,raginsky17a} and we provide finite-time error guarantees for the proposed algorithm. 

Apart from its nice theoretical properties, the proposed algorithm has also significant practical importance: (i) it has low computationally requirements and can be easily run on an everyday laptop CPU, (ii) it has a strong potential for privacy preserving applications since it only requires random projections of the data, rather than the data itself. Our experiments on both synthetic and real datasets support our theory and show that our algorithm is able to generate samples in challenging scenarios. 



% whatever the cost functional chosen. For instance, derivations based on the Wasserstein measure are difficult in the multidimensional case. 
% Here, we show how the \textit{sliced Wasserstein distance} written $\SW$ is a good candidate for the choice of the cost function in \eqref{eqn:gradflow}. $\SW$ was originally introduced in \cite{pitie2005n} -- this is not correct -- and was shown equivalent to Wasserstein distance in \cite{bonnotte2013unidimensional}. 

% Although we will define it more rigorously later, it may be understood as applying many scalar transportations of randomized projections of the data, for which analytical expressions are available. 
% Its most import feature is to allow the straightforward treatment of the primal OT problem, thus avoiding the delicate training brought in by the dual formulation considered by Wasserstein-GAN. -- I think this explanation is not accurate.




% \item Contributions of the current paper
% \begin{itemize}
% \item Development of a novel gradient flow for generative modeling purpose
% \item Establish the connections with SDEs
% \item Develop a practical way to simulate the SDE
% \item Establish theoretical guarantees
% \item Experimental validation
% \end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "nips_2018_sketchmcmc"
%%% End:
