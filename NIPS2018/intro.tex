%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Introduction}

% \begin{itemize}
% \item Short intro to implicit generative models
% \item Connection with optimal transport \cite{genevay2017gan}, several other papers
% \item Information about sliced-Wasserstein distance and usage in generative modeling \cite{bonnotte2013unidimensional,kolouri2018sliced,wu2017generative}
% \item Contributions of the current paper
% \begin{itemize}
% \item Development of a novel gradient flow for generative modeling purpose
% \item Establish the connections with SDEs
% \item Develop a practical way to simulate the SDE
% \item Establish theoretical guarantees
% \item Experimental validation
% \end{itemize}
% \end{itemize}

\umut{There are disconnected paragraphs, I'm still working on it.}

% implicit modeling

Implicit generative modeling \cite{diggle1984monte, mohamed2016learning} has become very popular recently and has proven successful in various fields. Variational auto-encoders (VAE \cite{kingma2013VAE}) and generative adversarial networks (GAN \cite{goodfellow2014generative}) are the two well-known examples.

The goal in these approaches can be briefly described as learning the underlying probability measure of a given dataset, denoted as $\nu \in \PS(\Omega_\nu)$, without assuming any explicit probability law for it.

Instead, the implicit generative framework models the data as the output $T(x)$ of a measurable map $T: \Omega_\mu \mapsto \Omega_\nu$, with inputs $x$ generated from a known and easy to sample source measure $\mu$ on $\Omega_\mu$ (e.g. with Gaussian or uniform i.i.d.\ entries). while its outputs should match the target measure $\nu$ on $\Omega_\nu$, which can be very complicated in real applications.
For the sake of simplicity, we only consider in this study the case where the target measure $\nu$ is not known in closed form, but rather only through $N$ elements from a learning database $D = \{y_1 , \dots , y_N \}$ that are assumed independently sampled from it. It is common in applications to have a very large amount of millions or billions of such samples.


% VAE and GAN quickly
In that context, learning generative networks witnessed several groundbreaking contributions in the recent years. First, variational auto-encoders (VAE \cite{kingma2013VAE}) were proposed that adopt a encoder-decoder strategy, for which the output of the encoder ought to match some prescribed $\mu$, while the combination of encoder and decoder should be as close as possible to identity when evaluated on the training data. The hope of such an approach is easily generate samples from $\nu$ by applying the decoder to new samples from $\mu$, and not only to encoded training points.%maybe a bit more of references ?

While VAE showed great potential in generative modeling, yet another important contribution came with Generative Adversarial Networks (GAN, \cite{goodfellow2014generative,salimans2016improved,eghbal2017probabilistic}), which take generative learning as an adversarial game. On the one hand, a discriminative system is trained to detect generated samples from those of the dataset. On the other hand, the generative network is trained so as to minimize the performance of the classifier. Optimization then alternates between both.

% now only talk about OT

While this strategy was found effective, theoretical foundations for it were highlighted only recently \cite{bousquet2017optimal} in an optimal transport (OT) setting as minimizing a Wasserstein distance between the output distribution and the target $\nu$. More specifically, GAN and VAE were shown to tackle the same OT problem in a dual way \cite{bousquet2017optimal,genevay2017gan}.

As it turns out, the venerable old topic of optimal transport brings new light on the generative modeling problem. In short, OT studies whether it is possible to transform samples from a source distribution $\mu$ to a target distribution $\nu$. From this perspective, an ideal generative model is simply a transport map from $\mu$ to $\nu$.
%
This can be explained using some `push-forward operators': we seek a mapping $T$ that `pushes $\mu$ onto $\nu$', and is formally defined as $\int_A \nu(dx) = \int_{T^{-1}(A)} \mu(dx) $ for all Borel $A \subset {\cal B}(\Omega_\nu)$. If this relation holds, we denote the push-forward operator $T_\#$, such that $T_\# \mu = \nu$. Provided very mild conditions on these distributions are respected (notably that $\mu$ is non-atomic \cite{villani2008optimal}), existence and uniqueness of such a transport map is guaranteed. However, it remains challenging to construct it in practice.

Most of the current generative modeling strategies consider an operator that belongs to a parametric family $T_{\phi}$ with $\phi \in \Phi$, and aims to find the best parameter $\phi^\star$ that would give $T_{\phi^\star \#}\mu \approx \nu$. This is typically achieved by attempting to minimize the following optimization problem:
\begin{align}
\phi^\star = \argmin_{\phi \in \Phi} \W(T_{\phi \#}\mu, \nu),
\end{align}
where $\W$ denotes the Wasserstein distance that will be properly defined in Section~\ref{sec:techbg}. Genevay et al.\ \cite{genevay2017gan} showed that Wasserstein GANs and VAEs both use this formulation with different parameterizations.

In this study, we follow a completely different approach and we consider a functional optimization problem, that is defined through gradient flows for measures in the Wasserstein space. Informally, the flow will have a shape as follows:
\begin{align}
\partial_t \mu_t = - \nabla_{\W} \Bigl\{ \mathrm{Cost}(\mu_t, \nu) + \mathrm{Reg}(\mu_t)\Bigr\} \label{eqn:gradflow}
\end{align}
where the functional $\mathrm{Cost}$ computes a discrepancy between $\mu_t$ and $\nu$, $\mathrm{Reg}$ denotes a regularization functional, and $\nabla_{\W}$ denotes a notion of gradient with respect to a probability density function in the $\W$ metric for probability measures\footnote{This gradient flow is similar to the the usual Euclidean gradient flows, i.e.\ $dx/dt = - \nabla (f(x) + r(x))$, where $f$ is typically the data-dependent cost function and $r$ is a regularization term. The (explicit) Euler discretization of this flow results in the well-known gradient descent algorithm for solving $\min_x (f(x)+r(x))$.}. If this flow can be simulated, one would hope for $\mu_t$ to converge to the minimum of the functional optimization problem: $\min_\mu ( \mathrm{Cost}(\mu, \nu) + \mathrm{Reg}(\mu)$).

However, in general these flows cannot be either solved or simulated perfectly. \umut{discuss $\SW$.}
