%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Regularized Sliced-Wasserstein Flows for Generative Modeling}

% \begin{itemize}
% \item Start from the flow given in \cite{bonnotte2013unidimensional} and discuss the limitations 
% \begin{itemize}
% \item No convergence guarantee
% \item Over-fitting risk when the target is a collections of Dirac masses
% \end{itemize}
% \item Motivation for the entropy-regularization (discuss the differences with the Sinkhorn distances \cite{genevay2018learning})
% \item Development of the new gradient flow -- first theoretical result
% \item Establish the distance between the stationary measures of the two gradient flows (second theoretical result)
% \item View the new flow as a non-linear Fokker-Planck equation -- establish the connection with the appropriate SDE
% \item Develop the numerical scheme 
% \item Analyze the TV distance between the measure at iteration $k$ and the invariant measure
% \item We don't need to see the data, we just need the projections. In addition to its computational implications, this can be crucial for privacy preserving applications
% \end{itemize}



% \begin{align}
% \partial_t \rho = - \nabla_{\W} \F(\rho) \label{eqn:gradflow}
% \end{align}
% where $\nabla_{\W}$ denotes a notion of gradient in the $\W$ metric and the functional $\F$ is chosen as the squared sliced-Wasserstein distance between $\mu$ and $\nu$:
% \begin{align}
% \F(\rho) \triangleq \frac1{2} \SW^2(\rho, \pi)
% \end{align}
% where $\pi$ denotes the density of $\nu$. Here we abused the notation by defining $\SW$ on the densities instead of the measures; we implicitly assume that both measures $\mu$ and $\nu$ are dominated by the Lebesgue measure. This gradient flow basically constructs a path $(\rho_t)_{t\geq 0}$ that minimizes $\F$ as $t$ increases. In other words, the goal in construction such a flow is to solve the following problem:
% \begin{align}
% \rho^\star = \argmin_\rho \F(\rho).
% \end{align}
% Since we obviously have $\rho^\star = \pi$, this gradient flow will start from $\rho_0$ and bring it closer to $\pi$ as $t$ evolves.

% By following \cite{santambrogio2017euclidean}, the gradient given in \eqref{eqn:gradflow} can be written as follows:
% \begin{align}
% \nabla_{\W} \F(\rho) = -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho}(\rho) \bigr) \Bigr), \label{eqn:gradw2}
% \end{align}
% where $\frac{\delta \F}{\delta \rho}(\rho)$ denotes the first variation of $\F$.

By this definition, $\mu_t$ should directly go to $\nu$. However, in practical settings, we will have finitely many samples from $\mu_0$ and $\nu$, therefore this scheme will somehow `overfit' to the data distribution. Therefore what we propose is to somehow `regularize' the gradient flow by introducing an entropy term to the minimization process. In particular, we modify the gradient flow given in \eqref{eqn:gradflow} as follows:
\begin{align}
\partial_t \rho = - \nabla_{\W} \Biggl\{ \F_\lambda(\rho) \triangleq  \frac1{2} \SW^2(\rho, \pi) + \lambda \He(\rho) \Biggr\},
\end{align}
where $\He(\rho) \triangleq \int (h \circ \rho) (x) dx $ denotes the negative entropy of $\rho$ with $h(t) = t \log t$. This regularization somewhat corresponds to assuming a Gaussian prior on the density $\rho$: when $\lambda$ goes to infinity, the optimal $\rho$ that minimizes $\F_\lambda$ will be a Gaussian density since the Gaussian densities have the maximum entropy.


This time the optimization problem is modified:
\begin{align}
\min_\rho \F_\lambda(\rho),
\end{align}
The idea in this new gradient flow formulation is to take $\rho_t$ as close as possible to $\pi$, while trying to keep its entropy at a certain level, so that it would be expressive for generative modeling purposes.

% We can now construct the modified gradient flow as follows:
% \begin{align}
% \partial_t \rho &= - \nabla_{\W} \F_\lambda(\rho) \\
% &=  \nabla \cdot (\rho \> v_t) + \lambda \Delta \rho. \label{eqn:gradflow_reg}
% \end{align}

% Here, we present our first theoretical result. 
% \begin{thm}
% The gradient flow is a valid gradient flow. \umut{Szymon.}
% \end{thm}

% We let $\mathcal{F}_{\lambda}(\mu) = \frac{1}{2} SW_2^2(\mu, \nu) + \lambda H(\mu)$ for a chosen reference measure $\nu$.

% \begin{lemma}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Fix a time step $h > 0$, regularization constant $\lambda > 0$ and a radius $r > \sqrt{d}$. For a probability measure $\mu_0$ on $B(0, r)$ with density $\rho_0 \in L^{\infty}$, there is a probability measure $\mu$ on $\overline{B}(0,r)$ minimizing:
% \[
% \mathcal{G}(\mu) = \mathcal{F}_{\lambda} (\mu) + \frac{1}{2h} W_2^2(\mu, \mu_0) 
% \]
% Moreover the optimal $\mu$ has a density $\rho$ on $B(0,r)$ and:
% \begin{equation} \label{ineq:inf_norm_bound}
% ||\rho||_{L^{\infty}} \leq (1 + h/\sqrt{d})^d ||\rho_0||_{L^{\infty}}
% \end{equation}
% \end{lemma}

% \begin{thm} \label{thm:existance_gmm_scheme}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Choose a regularization constant $\lambda > 0$ and radius $r > \sqrt{d}$. Given an absolutely continuous measure $\mu_0 \in \mathcal{P}(\cB(0,r))$ with density $\rho_0 \in L^p$, there is a Lipschitz generalized minimizing movement scheme $(\mu_t)_{t\geq 0}$ in $\mathcal{P}(\cB(0,r))$ starting from $\mu_0$ for the functional:
% \[
% \mathcal{G}(h, k , \mu_+, \mu_-) = \mathcal{F}_{\lambda}(\mu_+) + \frac{1}{2h}\W(\mu_+, \mu_-)
% \]
% Morover for time $t > 0$ measure $\mu_t$ has density $\rho_t$ and:
% % \[
% $||\rho_t||_{L^p} \leq e^{t\sqrt{d}}/q ||\rho_0||_{L^p}$.
% % \]
% \end{thm}


\begin{thm}
Let $\mu_t$ be a generalized minimizing movement scheme given by \ref{thm:existance_gmm_scheme}. We denote by $\rho_t$ the density of $\mu_t$. Then $\rho_t$ satisfies the continuity equation:
\begin{align}
\frac{\partial \rho_t}{\partial t} + \nabla \cdot (v_t \rho_t) = \lambda \Delta \rho_t  \quad \quad \quad v_t(x) = - \int_{\Sp^{d-1}} \psi_{t, \theta}'(\langle x , \theta \rangle ) \theta d\theta  \label{eqn:gradflow_reg}
\end{align}
in a weak sense. Here, $\psi_{t,\theta}$ denotes the Kantorovich potential between $\theta^*_{\#}\mu_t$ and $\theta^*_{\#}\nu$. 
\end{thm}

% Bonnotte \cite{bonnotte2013unidimensional} considers the IDT algorithm \cite{pitie2005n} and develops a continuity equation, given as follows:
% \begin{align}
% \partial_t \rho_t - \nabla \cdot (\rho_t v_t) = 0,
% \end{align}
% $\rho_t$ is the density of $\mu_t$ and

% In fact, one can show that, this is nothing but a gradient flow in the Wasserstein spaces, given as follows:


\section{Connection with Stochastic Differential Equations}

We now consider the modified flow given in \eqref{eqn:gradflow_reg}. We can observe that, this equation is a Fokker-Planck-type equation that has a probabilistic counterpart. More precisely, the following stochastic differential equation (SDE):
\begin{align}
d X_t = - v_t(X_t) dt + \sqrt{2 \lambda } d W_t, \label{eqn:sde}
\end{align}
where $W_t$ denotes the standard Brownian motion, is associated with \eqref{eqn:gradflow_reg}, in the sense that the probability distribution of the solution process $(X_t)_{t\geq0}$ satisfies the flow \eqref{eqn:gradflow_reg}.  



\subsection{Particle system for approximating the SDE and the Euler-Maruyama discretization}

\begin{wrapfigure}{R}{0.52\textwidth}
% \vspace{-15pt}
    \begin{minipage}{0.52\textwidth}
     \begin{algorithm2e} [H]
		 \SetInd{0.1ex}{1.5ex}
		 \DontPrintSemicolon
		 \SetKwInOut{Input}{input}
		 \SetKwInOut{Output}{output}
		 \Input{${\cal D} \equiv \{y_i\}_{i=1}^P$, $\mu_0$, $N$, $N_\theta$, $h$, $\lambda$}
		 \Output{$\{\bar{X}_K^i\}_{i=1}^N$}
		 {\color{purple} \small \tcp{Initialize the particles}}
		 $\bar{X}_0^i \simiid \mu_0$, \hfill $i = 1,\dots,N$ \\%{\color{purple} \small \tcp{Initialize the particles}}  
		 \For{$k = 0,\dots K-1$}{
		    {\color{purple} \small \tcp{Generate random directions}}
		    $\theta_n \sim \mathrm{Uniform}(\Sp^{d-1})$, \hfill $n = 1,\dots,N_\theta$\\
		    {\color{purple} \small \tcp{Compute the projections}}
		    $\theta_{n\#\bar{\mu}^N_{kh}}^* = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle}$, \hfill $n = 1,\dots,N_\theta$ \\ 
		    $\theta_{n\#\nu}^* = \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, \hfill $n = 1,\dots,N_\theta$ \\
		    % {\color{mydarkblue} \tcp{Generate the new iterate \eqref{eqn:update_th_ult}}}
		    % Generate the new iterate: 
		    {\color{purple} \small \tcp{Update the particles}}
		    $\bar{X}_{k+1}^i = \bar{X}_{k}^i - h \hspace{0.5pt} \hat{v}(\bar{X}^i_k, \bar{\mu}_{kh}^N ) + \sqrt{2 \lambda h} Z^i_{k+1}$ \vspace{2pt} \\
		    $\hfill i = 1,\dots,N$
		 }
		 \caption{Sliced-Wasserstein Flow}
		 \label{algo:flow}
	 \end{algorithm2e}
\end{minipage}
\vspace{-20pt}
\end{wrapfigure}

If we could simulate \eqref{eqn:sde}, then we could use the sample paths $(X_t)_t$ as
samples drawn from $\nu_\lambda$, which is not very far from $\nu$. However, in practice this is not possible due to two reasons: (i) the drift $v_t$ cannot be computed analytically since it depends on the measure at time $t$, i.e.\ $\mu_t$, (ii) the SDE \eqref{eqn:sde} is a continuous-time process, it needs to be discretized.  

We now focus on the first issue. In order to highlight the fact that the drift in \eqref{eqn:sde} depends on $\mu_t$, within this section we will denote the drift as $v_t(x) \equiv v(x, \mu_t)$. With this notation, it can be clearly observed that the SDE \eqref{eqn:sde} is similar to McKean-Vlasov SDEs \cite{veretennikov2006ergodic,mishura2016existence} whose drift depends on the distribution of $X_t$. By using this connection, we borrow tools from the relevant SDE literature \cite{malrieu03,cgm-08} for developing an approximate simulation method for \eqref{eqn:sde}. 

Our approach is based on defining a \emph{particle system} that serves as an approximation to the original SDE \eqref{eqn:sde}, and is given as follows:
\begin{align}
d X_t^i = - v(X_t^i, \mu_t^{(N)}) dt + \sqrt{2 \lambda } d W_t^i \> , \quad i = 1,\dots, N, \label{eqn:sde_particle}
\end{align}
where $i$ denotes the particle index, $N \in \mathbb{N}_+$ denotes the total number of particles, and $\mu_t^{(N)}$ denotes the law of the particles $\{X_t^j\}_{j=1}^N$. This particle system is particularly interesting, since (i) one typically has $\lim_{N \rightarrow \infty} \mu_t^{(N)}= \mu_t $ and the distance between $\mu_t^{(N)}$ and $\mu_t$ is of order ${\cal O}(1/\sqrt{N})$ for $t$ large enough \cite{malrieu03,cgm-08}, and (ii) each of the particle systems in \eqref{eqn:sde_particle} can be simulated by using an Euler-Maruyama discretization scheme. We note that the existing theoretical results in \cite{veretennikov2006ergodic,mishura2016existence} do not directly apply to our case due to the non-standard form of our drift. However, we conjecture that a similar result holds for our problem as well. Proving such a result is technically very involved and we leave it out of the scope of this study. %We will support this claim with experimental results.

We now consider the approximate Euler-Maruyama discretization for each particle SDE, that is given as follows:
\begin{align}
\bar{X}^i_{k+1} = \bar{X}^i_k - h \hspace{0.5pt} \hat{v}(\bar{X}^i_k, \bar{\mu}_{kh}^N ) + \sqrt{2 \lambda h} Z^i_{k+1},
\end{align}
where $k \in \mathbb{N}_+$ denotes the iteration number, for each particle $i$, $\{Z^i_k\}_{k}$ denotes a series of standard Gaussian random variables in $\R^d$, $h$ denotes the step-size, $\bar{\mu}_{kh}^{(N)}$ denotes the law of $\{\bar{X}_{k}^j\}_{j=1}^N$, and $\hat{v}$ is a computationally tractable estimator of $v$. Here, all the initial particles $\{\bar{X}_0^i\}_{i=1}^N$ are drawn from $\mu_0$ in an i.i.d.\ fashion.

We now define a computationally tractable estimator $\hat{v}$ of the drift, given as follows:
\begin{align}
\hat{v}(x,\mu) \triangleq - \frac1{N_\theta} \sum_{n=1}^{N_\theta} \psi_{t, \theta_n}'(\langle x , \theta_n \rangle ) \theta_n = \frac1{N_\theta} \sum_{n=1}^{N_\theta} \Bigl[ (F_{\theta_{n\#\mu}^*}^{-1} \circ F_{\theta_{n\#\nu}}) (x) - \langle \theta_n, x\rangle \Bigr] \theta_n , \label{eqn:approxdrift}
\end{align}
where $\{\theta_n\}_{n=1}^{N_\theta}$ denotes a collection of i.i.d.\ samples that are drawn uniformly on $\Sp^{d-1}$, $F_{\theta_{n\#\mu}^*}$ and $F_{\theta_{n\#\nu}^*}$ denote the (one-dimensional) CDFs of $\theta_{n\#\mu}^*$ and $\theta_{n\#\nu}^*$, respectively. In \eqref{eqn:approxdrift} we used the definition of the derivative of the one-dimensional Kantorovich potential, which was defined in Section~\ref{sec:sw}. 




Given the fact that, in practical applications, $\bar{\mu}^N_{kh}$ will be a collection of Dirac measures, i.e.\ $\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\bar{X}_{k}^i}$, we directly have $\theta_{n\#\bar{\mu}^N_{kh}}^* = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle} $, that is the one-dimensional empirical distribution of the projected samples $\{\langle \theta_n, \bar{X}_{k}^i \rangle\}_{i=1}^N$. Similarly, for a given dataset ${\cal D} \equiv \{y_i\}_{i=1}^P$, we have $\theta_{n\#\nu}^* = \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, that is the one-dimensional empirical distribution of the projected data points $\{\langle \theta_n, y^i \rangle\}_{i=1}^P$. Once the projections of the particles and the data points are computed along the directions $\{\theta_n\}_n$, it is straightforward to approximately compute the function $(F_{\theta_{n\#\bar{\mu}}^*}^{-1} \circ F_{\theta_{n\#\nu}}) $ by using standard techniques that are based on sorting and linear interpolation \umut{cite}, whose details are given in the supplementary document. The overall algorithm is illustrated in Algorithm~\ref{algo:flow}.


Even though the error induced by this approximation scheme can be incorporated into our current analysis framework, we choose to neglect this error, since these one-dimensional computations can be done very accurately. Therefore, $\hat{v}$ turns out to be an unbiased estimator of $v$, i.e.\ $\E[\hat{v}(x,\mu)] = v(x,\mu)$, for any $x\in \Omega$ and $\mu \in \PS(\Omega)$, where the expectation is taken over $\theta_n$.

\umut{If we generate the directions altogether, we can compute the projections of the data points as a preprocessing step, then we don't need to use the data directly.}










