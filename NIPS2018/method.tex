%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Regularized Sliced-Wasserstein Flows for Generative Modeling}

\subsection{Construction of the gradient flow}

% \begin{itemize}
% \item Start from the flow given in \cite{bonnotte2013unidimensional} and discuss the limitations 
% \begin{itemize}
% \item No convergence guarantee
% \item Over-fitting risk when the target is a collections of Dirac masses
% \end{itemize}
% \item Motivation for the entropy-regularization (discuss the differences with the Sinkhorn distances \cite{genevay2018learning})
% \item Development of the new gradient flow -- first theoretical result
% \item Establish the distance between the stationary measures of the two gradient flows (second theoretical result)
% \item View the new flow as a non-linear Fokker-Planck equation -- establish the connection with the appropriate SDE
% \item Develop the numerical scheme 
% \item Analyze the TV distance between the measure at iteration $k$ and the invariant measure
% \item We don't need to see the data, we just need the projections. In addition to its computational implications, this can be crucial for privacy preserving applications
% \end{itemize}



% \begin{align}
% \partial_t \rho = - \nabla_{\W} \F(\rho) \label{eqn:gradflow}
% \end{align}
% where $\nabla_{\W}$ denotes a notion of gradient in the $\W$ metric and the functional $\F$ is chosen as the squared sliced-Wasserstein distance between $\mu$ and $\nu$:
% \begin{align}
% \F(\rho) \triangleq \frac1{2} \SW^2(\rho, \pi)
% \end{align}
% where $\pi$ denotes the density of $\nu$. Here we abused the notation by defining $\SW$ on the densities instead of the measures; we implicitly assume that both measures $\mu$ and $\nu$ are dominated by the Lebesgue measure. This gradient flow basically constructs a path $(\rho_t)_{t\geq 0}$ that minimizes $\F$ as $t$ increases. In other words, the goal in construction such a flow is to solve the following problem:
% \begin{align}
% \rho^\star = \argmin_\rho \F(\rho).
% \end{align}
% Since we obviously have $\rho^\star = \pi$, this gradient flow will start from $\rho_0$ and bring it closer to $\pi$ as $t$ evolves.

% By following \cite{santambrogio2017euclidean}, the gradient given in \eqref{eqn:gradflow} can be written as follows:
% \begin{align}
% \nabla_{\W} \F(\rho) = -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho}(\rho) \bigr) \Bigr), \label{eqn:gradw2}
% \end{align}
% where $\frac{\delta \F}{\delta \rho}(\rho)$ denotes the first variation of $\F$.

In this section, we will construct a gradient flow in $\WS$, which will form the basis of the proposed nonparametric generative model. We begin by stating the functional optimization problem on which the gradient flow will be constructed:
\begin{align}
\min_\mu \Bigl\{ \F_\lambda(\mu) \triangleq  \frac1{2} \SW^2(\mu, \nu) + \lambda \He(\mu) \Bigr\}, \label{eqn:sw_optim}
\end{align} 
where $\He(\mu) \triangleq \int \rho(x) \log \rho(x) dx $ denotes the negative entropy of $\mu(dx) = \rho(x)dx$. The main idea in this problem is to find a measure $\mu^\star$ that is close to $\nu$ as much as possible and also has a certain amount of entropy to make sure that it is sufficiently expressive for generative modeling purposes. 

The importance of the entropy regularization becomes prominent in practical applications where we have finitely many samples drawn from $\nu$. In such a circumstance, the entropy regularization would prevent $\mu^\star$ to collapse on the data points and therefore avoid `over-fitting' to the data distribution. 

%Informally, the regularization corresponds to assuming a Gaussian prior on $\mu$: when $\lambda$ goes to infinity, $\mu^\star$ will converge to a Gaussian distribution since the Gaussian densities have the maximum entropy. \umut{discuss the difference between Sinkhorn.}

In the light of the optimization problem \eqref{eqn:sw_optim}, we will construct the gradient flow $\partial_t \mu_t = - \nabla_{\W} \F_\lambda(\mu_t)$. This gradient flow will be aptly called \emph{sliced-Wasserstein flows}. The main interest in such a gradient flow is that, for a curve $(\mu_t)_t$ satisfying the gradient flow, we would like to identify the PDE that is solved by $(\rho_t)_t$, where $\rho_t$ denotes the density of $\mu_t$.

We now state our first theoretical main result, that establishes the connection between the gradient flow and PDE that describes the evolution of the densities. 
\begin{thm}
\label{thm:continuity}
Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density, where $\cB(0,a)$ denotes the closed unit ball with center $0$ and radius $a$. Choose a regularization constant $\lambda > 0$ and radius $r > \sqrt{d}$. Assume that $\mu_0 \in \mathcal{P}(\cB(0,r))$ is absolutely continuous with density $\rho_0 \in L^2$. Let $\mu_t$ be a generalized minimizing movement scheme given by Theorem~\umut{num} in the supplementary document. Then, $\rho_t$ satisfies the following continuity equation:
\begin{align}
\frac{\partial \rho_t}{\partial t}   = -\nabla \cdot (v_t \rho_t) + \lambda \Delta \rho_t,  \quad \quad \quad v_t(x) \triangleq v(x,\mu_t) = - \int_{\Sp^{d-1}} \psi_{t, \theta}'(\langle x , \theta \rangle ) \theta d\theta  \label{eqn:gradflow_reg}
\end{align}
in a weak sense. Here, $\Delta$ denotes the Laplacian operator and $\psi_{t,\theta}$ denotes the Kantorovich potential between $\theta^*_{\#}\mu_t$ and $\theta^*_{\#}\nu$. 
\end{thm}
The proof of Theorem~\ref{thm:continuity} as well as the proofs of all our theoretical results are given in the supplementary document. For proving this theorem, we use the technique introduced in \cite{bonnotte2013unidimensional}: we first prove the existence of the gradient flow by showing that the solution curve $(\mu_t)_t$ is a limit of the solution of a time-discretized problem \cite{jordan1998variational}. Then we prove that the curve $(\rho_t)_t$ solves the PDE given in \eqref{eqn:gradflow_reg}. 


% We can now construct the modified gradient flow as follows:
% \begin{align}
% \partial_t \rho &= - \nabla_{\W} \F_\lambda(\rho) \\
% &=  \nabla \cdot (\rho \> v_t) + \lambda \Delta \rho. \label{eqn:gradflow_reg}
% \end{align}

% Here, we present our first theoretical result. 
% \begin{thm}
% The gradient flow is a valid gradient flow. \umut{Szymon.}
% \end{thm}

% We let $\mathcal{F}_{\lambda}(\mu) = \frac{1}{2} SW_2^2(\mu, \nu) + \lambda H(\mu)$ for a chosen reference measure $\nu$.

% \begin{lemma}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Fix a time step $h > 0$, regularization constant $\lambda > 0$ and a radius $r > \sqrt{d}$. For a probability measure $\mu_0$ on $B(0, r)$ with density $\rho_0 \in L^{\infty}$, there is a probability measure $\mu$ on $\overline{B}(0,r)$ minimizing:
% \[
% \mathcal{G}(\mu) = \mathcal{F}_{\lambda} (\mu) + \frac{1}{2h} W_2^2(\mu, \mu_0) 
% \]
% Moreover the optimal $\mu$ has a density $\rho$ on $B(0,r)$ and:
% \begin{equation} \label{ineq:inf_norm_bound}
% ||\rho||_{L^{\infty}} \leq (1 + h/\sqrt{d})^d ||\rho_0||_{L^{\infty}}
% \end{equation}
% \end{lemma}

% \begin{thm} \label{thm:existance_gmm_scheme}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Choose a regularization constant $\lambda > 0$ and radius $r > \sqrt{d}$. Given an absolutely continuous measure $\mu_0 \in \mathcal{P}(\cB(0,r))$ with density $\rho_0 \in L^p$, there is a Lipschitz generalized minimizing movement scheme $(\mu_t)_{t\geq 0}$ in $\mathcal{P}(\cB(0,r))$ starting from $\mu_0$ for the functional:
% \[
% \mathcal{G}(h, k , \mu_+, \mu_-) = \mathcal{F}_{\lambda}(\mu_+) + \frac{1}{2h}\W(\mu_+, \mu_-)
% \]
% Morover for time $t > 0$ measure $\mu_t$ has density $\rho_t$ and:
% % \[
% $||\rho_t||_{L^p} \leq e^{t\sqrt{d}}/q ||\rho_0||_{L^p}$.
% % \]
% \end{thm}




% Bonnotte \cite{bonnotte2013unidimensional} considers the IDT algorithm \cite{pitie2005n} and develops a continuity equation, given as follows:
% \begin{align}
% \partial_t \rho_t - \nabla \cdot (\rho_t v_t) = 0,
% \end{align}
% $\rho_t$ is the density of $\mu_t$ and

% In fact, one can show that, this is nothing but a gradient flow in the Wasserstein spaces, given as follows:


\subsection{Connection with stochastic differential equations}

Let us consider the PDE given in \eqref{eqn:gradflow_reg}. We observe that this equation is a Fokker-Planck-type equation \cite{bogachev2015fokker} that has a well-known probabilistic counterpart that can be expressed as a stochastic differential equation (SDE). More precisely, let us consider a stochastic process $(X_t)_{t\geq0}$, that is the solution following SDE:
\begin{align}
d X_t = v(X_t,\mu_t) dt + \sqrt{2 \lambda } d W_t, \qquad X_0 \sim \mu_0 \label{eqn:sde}
\end{align}
where $W_t$ denotes the standard Brownian motion. Then, the probability distribution of $X_t$ at time $t$ solves the PDE given in \eqref{eqn:gradflow_reg}. This informally means that, if we could simulate \eqref{eqn:sde}, then the distribution of $X_t$ would converge to $\nu_\lambda$, therefore, we could use the sample paths $(X_t)_t$ as samples drawn from $\nu_\lambda$. However, in practice this is not possible due to two reasons: (i) the drift $v_t$ cannot be computed analytically since it depends on the probability distribution of $X_t$, (ii) the SDE \eqref{eqn:sde} is a continuous-time process, it needs to be discretized.  


% \subsection{Particle system for approximating the SDE and the Euler-Maruyama discretization}





We now focus on the first issue. 
% In order to highlight the fact that the drift in \eqref{eqn:sde} depends on the distribution of $X_t$, denoted by $\mu_t$, within this section we will denote the drift as $v_t(x) \equiv v(x, \mu_t)$. 
% With this notation, it 
We can clearly observe that the SDE \eqref{eqn:sde} is similar to McKean-Vlasov SDEs \cite{veretennikov2006ergodic,mishura2016existence}, a family of SDEs whose drift depends on the distribution of $X_t$. By using this connection, we can borrow tools from the relevant SDE literature \cite{malrieu03,cgm-08} for developing an approximate simulation method for \eqref{eqn:sde}. 

Our approach is based on defining a \emph{particle system} that serves as an approximation to the original SDE \eqref{eqn:sde}. The particle system can be written as a collection of SDEs, given as follows \cite{bossy1997stochastic}:
\begin{align}
d X_t^i = v(X_t^i, \mu_t^{N}) dt + \sqrt{2 \lambda } d W_t^i \> , \quad i = 1,\dots, N, \label{eqn:sde_particle}
\end{align}
where $i$ denotes the particle index, $N \in \mathbb{N}_+$ denotes the total number of particles, and $\mu_t^{(N)}$ denotes the law of the particles $\{X_t^j\}_{j=1}^N$. This particle system is particularly interesting, since (i) one typically has $\lim_{N \rightarrow \infty} \mu_t^{N}= \mu_t $ with a rate of convergence of order ${\cal O}(1/\sqrt{N})$ for $t$ large enough \cite{malrieu03,cgm-08}, and (ii) each of the particle systems in \eqref{eqn:sde_particle} can be simulated by using an Euler-Maruyama discretization scheme. We note that the existing theoretical results in \cite{veretennikov2006ergodic,mishura2016existence} do not directly apply to our case due to the non-standard form of our drift. However, we conjecture that a similar result holds for our problem as well. Proving such a result would be very involved and we leave it out of the scope of this study. %We will support this claim with experimental results.

\subsection{Approximate Euler-Maruyama discretization}


In order to be able to simulate the particle SDEs \ref{eqn:sde_particle}, in this section, we now an approximate Euler-Maruyama discretization for each particle SDE. The proposed scheme is given as follows:
\begin{align}
\bar{X}^i_0 \simiid \mu_0, \quad \qquad \bar{X}^i_{k+1} = \bar{X}^i_k + h \hspace{0.5pt} \hat{v}(\bar{X}^i_k, \bar{\mu}_{kh}^N ) + \sqrt{2 \lambda h} Z^i_{k+1}, \quad \qquad  \forall i \in  \{1,\dots,N\} \label{eqn:euler_particle}
\end{align}
where $k \in \mathbb{N}_+$ denotes the iteration number, for each particle $i$, $\{Z^i_k\}_{k}$ denotes a series of standard Gaussian random variables in $\R^d$, $h$ denotes the step-size, $\bar{\mu}_{kh}^{(N)}$ denotes the law of $\{\bar{X}_{k}^j\}_{j=1}^N$, and $\hat{v}$ is a computationally tractable estimator of $v$. Here, all the initial particles $\{\bar{X}_0^i\}_{i=1}^N$ are drawn from $\mu_0$ in an i.i.d.\ fashion.
%
The function $\hat{v}$ is a computationally tractable estimator of the original drift $v$, and is defined as follows:
\begin{align}
\hat{v}(x,\mu) \triangleq - \frac1{N_\theta} \sum_{n=1}^{N_\theta} \psi_{t, \theta_n}'(\langle x , \theta_n \rangle ) \theta_n = \frac1{N_\theta} \sum_{n=1}^{N_\theta} \Bigl[ (F^{-1}_{\theta^*_{n\#}\mu} \circ F_{\theta^*_{n\#}\nu}) (\langle \theta_n, x\rangle) - \langle \theta_n, x\rangle \Bigr] \theta_n , \label{eqn:approxdrift}
\end{align}
where $\{\theta_n\}_{n=1}^{N_\theta}$ denotes a collection of i.i.d.\ samples that are drawn uniformly on $\Sp^{d-1}$, $F_{\theta^*_{n\#}\mu}$ and $F_{\theta^*_{n\#}\nu}$ denote the (one-dimensional) CDFs of the distributions $\theta^*_{n\#}\mu$ and $\theta^*_{n\#}\nu$, respectively. In \eqref{eqn:approxdrift} we used the definition of the derivative of the one-dimensional Kantorovich potential, which was defined in Section~\ref{sec:sw}. 


\begin{wrapfigure}{R}{0.52\textwidth}
\vspace{-15pt}
    \begin{minipage}{0.52\textwidth}
     \begin{algorithm2e} [H]
		 \SetInd{0.1ex}{1.5ex}
		 \DontPrintSemicolon
		 \SetKwInOut{Input}{input}
		 \SetKwInOut{Output}{output}
		 \Input{${\cal D} \equiv \{y_i\}_{i=1}^P$, $\mu_0$, $N$, $N_\theta$, $h$, $\lambda$}
		 \Output{$\{\bar{X}_K^i\}_{i=1}^N$}
		 {\color{purple} \small \tcp{Initialize the particles}}
		 $\bar{X}_0^i \simiid \mu_0$, \hfill $i = 1,\dots,N$ \\%{\color{purple} \small \tcp{Initialize the particles}}  
		 \For{$k = 0,\dots K-1$}{
		    {\color{purple} \small \tcp{Generate random directions}}
		    $\theta_n \sim \mathrm{Uniform}(\Sp^{d-1})$, \hfill $n = 1,\dots,N_\theta$\\
		    {\color{purple} \small \tcp{Compute the projections}}
		    $\theta^*_{n\#}\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle}$, \hfill $n = 1,\dots,N_\theta$ \\ 
		    $\theta^*_{n\#}\nu = \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, \hfill $n = 1,\dots,N_\theta$ \\
		    % {\color{mydarkblue} \tcp{Generate the new iterate \eqref{eqn:update_th_ult}}}
		    % Generate the new iterate: 
		    {\color{purple} \small \tcp{Update the particles}}
		    $\bar{X}_{k+1}^i = \bar{X}_{k}^i - h \hspace{0.5pt} \hat{v}(\bar{X}^i_k, \bar{\mu}_{kh}^N ) + \sqrt{2 \lambda h} Z^i_{k+1}$ \vspace{2pt} \\
		    $\hfill i = 1,\dots,N$
		 }
		 \caption{Sliced-Wasserstein Flow}
		 \label{algo:flow}
	 \end{algorithm2e}
\end{minipage}
\vspace{-20pt}
\end{wrapfigure}

% \newcommand{\shr}{{\lserif\#}}

Given the fact that $\bar{\mu}^N_{kh}$ will be a collection of Dirac measures in practical applications, i.e.\ $\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\bar{X}_{k}^i}$, we directly have $\theta^*_{n\#}\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle} $, that is the one-dimensional empirical distribution of the projected samples $\{\langle \theta_n, \bar{X}_{k}^i \rangle\}_{i=1}^N$. From a similar point of view, for a given dataset ${\cal D} \equiv \{y_i\}_{i=1}^P$, we assume that we observe $P$ i.i.d.\ random samples from $\nu$. Therefore, we can use these samples to approximate $\theta^*_{n\#}\nu$, such that we have $\theta^*_{n\#}\nu \approx \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, that is the one-dimensional empirical distribution of the projected data points $\{\langle \theta_n, y^i \rangle\}_{i=1}^P$. We believe that this approximation would be accurate in practical applications since it is currently common to have a very large datasets.


Once the projections of the particles and the data points are computed along the directions $\{\theta_n\}_n$, it is straightforward to approximately compute the function $(F_{\theta^*_{n\#}\bar{\mu}}^{-1} \circ F_{\theta^*_{n\#}\nu}) $ by using standard techniques that are based on linear interpolation \umut{cite}, whose details are given in the supplementary document. The overall algorithm is illustrated in Algorithm~\ref{algo:flow}.


Even though the error induced by this approximation scheme can be incorporated into our current analysis framework, we choose to neglect this error, since these one-dimensional computations can be done very accurately. This renders $\hat{v}$ to be an unbiased estimator of $v$, i.e.\ $\E[\hat{v}(x,\mu)] = v(x,\mu)$, for any $x\in \Omega$ and $\mu \in \PS(\Omega)$, where the expectation is taken over $\theta_n$.

\umut{If we generate the directions altogether, we can compute the projections of the data points as a preprocessing step, then we don't need to use the data directly.}










