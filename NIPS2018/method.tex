%!TEX root = ./nips_2018_sketchmcmc.tex

\section{Regularized Sliced-Wasserstein Flows for Generative Modeling}

\subsection{Construction of the gradient flow}

% \begin{itemize}
% \item Start from the flow given in \cite{bonnotte2013unidimensional} and discuss the limitations
% \begin{itemize}
% \item No convergence guarantee
% \item Over-fitting risk when the target is a collections of Dirac masses
% \end{itemize}
% \item Motivation for the entropy-regularization (discuss the differences with the Sinkhorn distances \cite{genevay2018learning})
% \item Development of the new gradient flow -- first theoretical result
% \item Establish the distance between the stationary measures of the two gradient flows (second theoretical result)
% \item View the new flow as a non-linear Fokker-Planck equation -- establish the connection with the appropriate SDE
% \item Develop the numerical scheme
% \item Analyze the TV distance between the measure at iteration $k$ and the invariant measure
% \item We don't need to see the data, we just need the projections. In addition to its computational implications, this can be crucial for privacy preserving applications
% \end{itemize}



% \begin{align}
% \partial_t \rho = - \nabla_{\W} \F(\rho) \label{eqn:gradflow}
% \end{align}
% where $\nabla_{\W}$ denotes a notion of gradient in the $\W$ metric and the functional $\F$ is chosen as the squared sliced-Wasserstein distance between $\mu$ and $\nu$:
% \begin{align}
% \F(\rho) \triangleq \frac1{2} \SW^2(\rho, \pi)
% \end{align}
% where $\pi$ denotes the density of $\nu$. Here we abused the notation by defining $\SW$ on the densities instead of the measures; we implicitly assume that both measures $\mu$ and $\nu$ are dominated by the Lebesgue measure. This gradient flow basically constructs a path $(\rho_t)_{t\geq 0}$ that minimizes $\F$ as $t$ increases. In other words, the goal in construction such a flow is to solve the following problem:
% \begin{align}
% \rho^\star = \argmin_\rho \F(\rho).
% \end{align}
% Since we obviously have $\rho^\star = \pi$, this gradient flow will start from $\rho_0$ and bring it closer to $\pi$ as $t$ evolves.

% By following \cite{santambrogio2017euclidean}, the gradient given in \eqref{eqn:gradflow} can be written as follows:
% \begin{align}
% \nabla_{\W} \F(\rho) = -\nabla \cdot \Bigl( \rho \nabla \bigl( \frac{\delta \F}{\delta \rho}(\rho) \bigr) \Bigr), \label{eqn:gradw2}
% \end{align}
% where $\frac{\delta \F}{\delta \rho}(\rho)$ denotes the first variation of $\F$.

In this section, we will construct a gradient flow in $\WS$, which will form the basis of the proposed nonparametric generative model. We begin by stating the functional optimization problem on which the gradient flow will be constructed:
\begin{align}
\min_\mu \Bigl\{ \F_\lambda(\mu) \triangleq  \frac1{2} \SW^2(\mu, \nu) + \lambda \He(\mu) \Bigr\}, \label{eqn:sw_optim}
\end{align}
where $\He(\mu) \triangleq \int \rho(x) \log \rho(x) dx $ denotes the negative entropy of $\mu(dx) = \rho(x)dx$. The main idea in this problem is to find a measure $\mu^\star$ that is close to $\nu$ as much as possible and also has a certain amount of entropy to make sure that it is sufficiently expressive for generative modeling purposes.

The importance of the entropy regularization becomes prominent in practical applications where we have finitely many samples drawn from $\nu$. In such a circumstance, the entropy regularization would prevent $\mu^\star$ to collapse on the data points and therefore avoid `over-fitting' to the data distribution.

%Informally, the regularization corresponds to assuming a Gaussian prior on $\mu$: when $\lambda$ goes to infinity, $\mu^\star$ will converge to a Gaussian distribution since the Gaussian densities have the maximum entropy. \umut{discuss the difference between Sinkhorn.}

In the light of the optimization problem \eqref{eqn:sw_optim}, we will construct the gradient flow $\partial_t \mu_t = - \nabla_{\W} \F_\lambda(\mu_t)$. This gradient flow will be aptly called \emph{sliced-Wasserstein flows}. The main interest in such a gradient flow is that, for a curve $(\mu_t)_t$ satisfying the gradient flow, we would like to identify the PDE that is solved by $(\rho_t)_t$, where $\rho_t$ denotes the density of $\mu_t$.

We now state our first theoretical main result, that establishes the connection between the gradient flow and PDE that describes the evolution of the densities.
\begin{thm}
\label{thm:continuity}
Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density, where $\cB(0,a)$ denotes the closed unit ball with center $0$ and radius $a$. Choose a regularization constant $\lambda > 0$ and radius $r > \sqrt{d}$. Assume that $\mu_0 \in \mathcal{P}(\cB(0,r))$ is absolutely continuous with density $\rho_0 \in L^2$. Let $\mu_t$ be a generalized minimizing movement scheme given by Theorem~\umut{num} in the supplementary document. Then, $\rho_t$ satisfies the following continuity equation:
\begin{align}
\frac{\partial \rho_t}{\partial t}   = -\nabla \cdot (v_t \rho_t) + \lambda \Delta \rho_t,  \quad \quad \quad v_t(x) \triangleq v(x,\mu_t) = - \int_{\Sp^{d-1}} \psi_{t, \theta}'(\langle x , \theta \rangle ) \theta d\theta  \label{eqn:gradflow_reg}
\end{align}
in a weak sense. Here, $\Delta$ denotes the Laplacian operator and $\psi_{t,\theta}$ denotes the Kantorovich potential between $\theta^*_{\#}\mu_t$ and $\theta^*_{\#}\nu$.
\end{thm}
The proof of Theorem~\ref{thm:continuity} as well as the proofs of all our theoretical results are given in the supplementary document. For proving this theorem, we use the technique introduced in \cite{bonnotte2013unidimensional}: we first prove the existence of the gradient flow by showing that the solution curve $(\mu_t)_t$ is a limit of the solution of a time-discretized problem \cite{jordan1998variational}. Then we prove that the curve $(\rho_t)_t$ solves the PDE given in \eqref{eqn:gradflow_reg}.


% We can now construct the modified gradient flow as follows:
% \begin{align}
% \partial_t \rho &= - \nabla_{\W} \F_\lambda(\rho) \\
% &=  \nabla \cdot (\rho \> v_t) + \lambda \Delta \rho. \label{eqn:gradflow_reg}
% \end{align}

% Here, we present our first theoretical result.
% \begin{thm}
% The gradient flow is a valid gradient flow. \umut{Szymon.}
% \end{thm}

% We let $\mathcal{F}_{\lambda}(\mu) = \frac{1}{2} SW_2^2(\mu, \nu) + \lambda H(\mu)$ for a chosen reference measure $\nu$.

% \begin{lemma}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Fix a time step $h > 0$, regularization constant $\lambda > 0$ and a radius $r > \sqrt{d}$. For a probability measure $\mu_0$ on $B(0, r)$ with density $\rho_0 \in L^{\infty}$, there is a probability measure $\mu$ on $\overline{B}(0,r)$ minimizing:
% \[
% \mathcal{G}(\mu) = \mathcal{F}_{\lambda} (\mu) + \frac{1}{2h} W_2^2(\mu, \mu_0)
% \]
% Moreover the optimal $\mu$ has a density $\rho$ on $B(0,r)$ and:
% \begin{equation} \label{ineq:inf_norm_bound}
% ||\rho||_{L^{\infty}} \leq (1 + h/\sqrt{d})^d ||\rho_0||_{L^{\infty}}
% \end{equation}
% \end{lemma}

% \begin{thm} \label{thm:existance_gmm_scheme}
% Let $\nu$ be a probability measure on $\cB(0,1)$ with a strictly positive smooth density. Choose a regularization constant $\lambda > 0$ and radius $r > \sqrt{d}$. Given an absolutely continuous measure $\mu_0 \in \mathcal{P}(\cB(0,r))$ with density $\rho_0 \in L^p$, there is a Lipschitz generalized minimizing movement scheme $(\mu_t)_{t\geq 0}$ in $\mathcal{P}(\cB(0,r))$ starting from $\mu_0$ for the functional:
% \[
% \mathcal{G}(h, k , \mu_+, \mu_-) = \mathcal{F}_{\lambda}(\mu_+) + \frac{1}{2h}\W(\mu_+, \mu_-)
% \]
% Morover for time $t > 0$ measure $\mu_t$ has density $\rho_t$ and:
% % \[
% $||\rho_t||_{L^p} \leq e^{t\sqrt{d}}/q ||\rho_0||_{L^p}$.
% % \]
% \end{thm}




% Bonnotte \cite{bonnotte2013unidimensional} considers the IDT algorithm \cite{pitie2005n} and develops a continuity equation, given as follows:
% \begin{align}
% \partial_t \rho_t - \nabla \cdot (\rho_t v_t) = 0,
% \end{align}
% $\rho_t$ is the density of $\mu_t$ and

% In fact, one can show that, this is nothing but a gradient flow in the Wasserstein spaces, given as follows:


\subsection{Connection with stochastic differential equations}

Let us consider the PDE given in \eqref{eqn:gradflow_reg}. We observe that this equation is a Fokker-Planck-type equation \cite{bogachev2015fokker} that has a well-known probabilistic counterpart that can be expressed as a stochastic differential equation (SDE). More precisely, let us consider a stochastic process $(X_t)_{t\geq0}$, that is the solution following SDE:
\begin{align}
d X_t = v(X_t,\mu_t) dt + \sqrt{2 \lambda } d W_t, \qquad X_0 \sim \mu_0 \label{eqn:sde}
\end{align}
where $W_t$ denotes the standard Brownian motion. Then, the probability distribution of $X_t$ at time $t$ solves the PDE given in \eqref{eqn:gradflow_reg}. This informally means that, if we could simulate \eqref{eqn:sde}, then the distribution of $X_t$ would converge to $\nu_\lambda$, therefore, we could use the sample paths $(X_t)_t$ as samples drawn from $\nu_\lambda$. However, in practice this is not possible due to two reasons: (i) the drift $v_t$ cannot be computed analytically since it depends on the probability distribution of $X_t$, (ii) the SDE \eqref{eqn:sde} is a continuous-time process, it needs to be discretized.


% \subsection{Particle system for approximating the SDE and the Euler-Maruyama discretization}





We now focus on the first issue.
% In order to highlight the fact that the drift in \eqref{eqn:sde} depends on the distribution of $X_t$, denoted by $\mu_t$, within this section we will denote the drift as $v_t(x) \equiv v(x, \mu_t)$.
% With this notation, it
We can clearly observe that the SDE \eqref{eqn:sde} is similar to McKean-Vlasov SDEs \cite{veretennikov2006ergodic,mishura2016existence}, a family of SDEs whose drift depends on the distribution of $X_t$. By using this connection, we can borrow tools from the relevant SDE literature \cite{malrieu03,cgm-08} for developing an approximate simulation method for \eqref{eqn:sde}.

Our approach is based on defining a \emph{particle system} that serves as an approximation to the original SDE \eqref{eqn:sde}. The particle system can be written as a collection of SDEs, given as follows \cite{bossy1997stochastic}:
\begin{align}
d X_t^i = v(X_t^i, \mu_t^{N}) dt + \sqrt{2 \lambda } d W_t^i \> , \quad i = 1,\dots, N, \label{eqn:sde_particle}
\end{align}
where $i$ denotes the particle index, $N \in \mathbb{N}_+$ denotes the total number of particles, and $\mu_t^N$ denotes the law of the particles $\{X_t^j\}_{j=1}^N$. This particle system is particularly interesting, since (i) one typically has $\lim_{N \rightarrow \infty} \mu_t^{N}= \mu_t $ with a rate of convergence of order ${\cal O}(1/\sqrt{N})$ for $t$ large enough \cite{malrieu03,cgm-08}, and (ii) each of the particle systems in \eqref{eqn:sde_particle} can be simulated by using an Euler-Maruyama discretization scheme. We note that the existing theoretical results in \cite{veretennikov2006ergodic,mishura2016existence} do not directly apply to our case due to the non-standard form of our drift. However, we conjecture that a similar result holds for our problem as well. Proving such a result would be very involved and we leave it out of the scope of this study. %We will support this claim with experimental results.

\subsection{Approximate Euler-Maruyama discretization}


In order to be able to simulate the particle SDEs \eqref{eqn:sde_particle} in practice, we propose an approximate Euler-Maruyama discretization for each particle SDE. First, all the initial particles $\{\bar{X}_0^i\}_{i=1}^N$ are drawn from $\mu_0$ in an i.i.d.\ fashion. Then, they are updated at each step $k$ following:
\begin{align}
\bar{X}^i_0 \simiid \mu_0, \quad \qquad \bar{X}^i_{k+1} = \bar{X}^i_k + h \hspace{0.5pt} \hat{v}_k(\bar{X}^i_k) + \sqrt{2 \lambda h} Z^i_{k+1}, \quad \qquad  \forall i \in  \{1,\dots,N\} \label{eqn:euler_particle}
\end{align}
where $k \in \mathbb{N}_+$ denotes the iteration number, $Z^i_k$ is a standard Gaussian random vector in $\R^d$, $h$ denotes the step-size, and $\hat{v}_k$ is a short-hand notation for a computationally tractable estimator of the original drift $v(\cdot, \bar{\mu}_{kh}^N)$, with $\bar{\mu}_{kh}^{(N)}$ being the law of $\{\bar{X}_{k}^j\}_{j=1}^N$. A question of fundamental practical importance is how to compute this function.

In this work, we approximate the integral given in \eqref{eqn:gradflow_reg} for the computation of $v$ by a finite sum using $N_\theta$ i.i.d samples from the sphere $\Sp^{d-1}$. At each iteration $k$, this is done by drawing $N_\theta$ such samples $\theta_{nk}$, and then by taking:

\begin{align}
\hat{v}_k(x) \triangleq - \frac1{N_\theta} \sum_{n=1}^{N_\theta} \psi_{k, \theta_{nk}}'(\langle x , \theta_{nk} \rangle ) \theta_{nk}, \label{eqn:approxdrift}
\end{align}

where $\psi_{k, \theta}'$  for any $\theta$ is the derivative of the one-dimensional Kantorovich potential defined in Section~\ref{sec:sw}, but applied to the OT problem from $\theta^*_\#\bar{\mu}_{kh}^{(N)}$ to $\theta^*_\#\nu$:
\begin{align}
   \psi_{k, \theta}'(z) = \Bigl[ z - (F^{-1}_{\theta^*_\#\bar{\mu}_{kh}^{(N)}} \circ F_{\theta^*_\#\nu}) (z)  \Bigr], \qquad \forall z \in \R.
 \end{align}
%where $\{\theta_n\}_{n=1}^{N_\theta}$ denotes a collection of i.i.d.\ samples that are drawn uniformly on $\Sp^{d-1}$, $F_{\theta^*_{n\#}\mu}$ and $F_{\theta^*_{n\#}\nu}$ denote the (one-dimensional) CDFs of the distributions $\theta^*_{n\#}\mu$ and $\theta^*_{n\#}\nu$, respectively. In \eqref{eqn:approxdrift} we used the definition of the derivative of the one-dimensional Kantorovich potential, which was defined in Section~\ref{sec:sw}.

\begin{wrapfigure}{R}{0.52\textwidth}
\vspace{-3pt}
    \begin{minipage}{0.52\textwidth}
     \begin{algorithm2e} [H]
		 \SetInd{0.1ex}{1.5ex}
		 \DontPrintSemicolon
		 \SetKwInOut{Input}{input}
		 \SetKwInOut{Output}{output}
		 \Input{${\cal D} \equiv \{y_i\}_{i=1}^P$, $\mu_0$, $N$, $N_\theta$, $h$, $\lambda$}
		 \Output{$\{\bar{X}_K^i\}_{i=1}^N$}
		 {\color{purple} \small \tcp{Initialize the particles}}
		 $\bar{X}_0^i \simiid \mu_0$, \hfill $i = 1,\dots,N$ \\
		 \For{$k = 0,\dots K-1$}
     {
		    {\color{purple} \small \tcp{Generate random directions}}
		    $\theta_{nk} \sim \mathrm{Uniform}(\Sp^{d-1})$, \hfill $n = 1,\dots,N_\theta$\\
        \For{$\theta\in\{\theta_{nk}\}_{n=1}^{N_\theta}$}
        {
        {\color{purple} \small \tcp{CDF of projected target}}
        $F_{\theta^*_\#\nu}=CDF\{\langle\theta,y_i\rangle\}_{i=1}^P$\\
        {\color{purple} \small \tcp{Quantiles of projected particles}}
        $F^{-1}_{\theta^*_\#\bar{\mu}_{kh}^{(N)}}=QF\{\langle\theta,\bar{X}_k^i\rangle\}_{i=1}^N$\\
          %$\theta^*_{n\#}\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle}$, \hfill $n = 1,\dots,N_\theta$ \\
          %$\theta^*_{n\#}\nu = \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, \hfill $n = 1,\dots,N_\theta$ \\
        }
        {\color{purple} \small \tcp{Update the particles}}
        $\bar{X}_{k+1}^i = \bar{X}_{k}^i - h \hspace{0.5pt} \hat{v}_k(\bar{X}^i_k) + \sqrt{2 \lambda h} Z^i_{k+1}$ \vspace{2pt} \\
        $\hfill i = 1,\dots,N$

		 }
		 \caption{Sliced-Wasserstein Flow}
		 \label{algo:flow}
	 \end{algorithm2e}
\end{minipage}
\vspace{-20pt}
\end{wrapfigure}

% \newcommand{\shr}{{\lserif\#}}
For any particular $\theta\in\Sp^{d-1}$, the CDF $F_{\theta^*_\#\nu}$ for the projection of the target distribution $\nu$ on $\theta$ can be straightforwardly computed from the data. This is done by first computing the projections $\langle \theta, y_i\rangle$ for all data points $y_i$, and then computing the empirical CDF for this set of $P$ scalars.

Similarly, the QF $F^{-1}_{\theta^*_\#\bar{\mu}_{kh}^{(N)}}$ for the particles at iteration $k$ is easy to compute: we first project all particles $\bar{X}_k^i$ to get $\langle \theta, \bar{X}_k^i\rangle$, and then compute the empirical quantile function of this set of $N$ scalar values.

In both cases, the true CDF and quantile functions are approximated as a linear interpolation between a set of the computed $Q\in\mathbb{N}$ empirical quantiles. In practice, we take $Q=100$ quantiles, which proved sufficient to approximate these functions faithfully.
Another source of approximation here comes from the fact that both the output distribution $\bar{\mu}_{kh}^{(N)}$ and the target $\nu$ will in practice be collections on Diracs centered on the observations $\bar{X}_k^i$ and $y_i$. Since it is currently common to have a very large datasets, we believe this approximation to be accurate in practice for the target. Concerning the number of particles, we observed $N=3000$ to be sufficient for most purposes in our experiments.

Even though the error induced by these approximation schemes can be incorporated into our current analysis framework, we choose to neglect it for now, because i/ all these one-dimensional computations can be done very accurately and ii/ quantization of the empirical CDF and QF can be modeled as additive Gaussian noise that enters our Maruyama discretization scheme \eqref{eqn:euler_particle}. This all means that $\hat{v}_k$ is an unbiased estimator of $v$, i.e.\ $\E[\hat{v}(x,\mu)] = v(x,\mu)$, for any $x\in \Omega$ and $\mu \in \PS(\Omega)$, where the expectation is taken over $\theta_{nk}$.

%Given the fact that $\bar{\mu}^N_{kh}$ will be a collection of Dirac measures in practical applications, i.e.\ $\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\bar{X}_{k}^i}$, we directly have $\theta^*_{n\#}\bar{\mu}^N_{kh} = \frac1{N} \sum_{i=1}^N \delta_{\langle \theta_n, \bar{X}_{k}^i \rangle} $, that is the one-dimensional empirical distribution of the projected samples $\{\langle \theta_n, \bar{X}_{k}^i \rangle\}_{i=1}^N$. From a similar point of view, for a given dataset ${\cal D} \equiv \{y_i\}_{i=1}^P$, we assume that we observe $P$ i.i.d.\ random samples from $\nu$. Therefore, we can use these samples to approximate $\theta^*_{n\#}\nu$, such that we have $\theta^*_{n\#}\nu \approx \frac1{N} \sum_{i=1}^P \delta_{\langle \theta_n, y_i \rangle} $, that is the one-dimensional empirical distribution of the projected data points $\{\langle \theta_n, y^i \rangle\}_{i=1}^P$. We believe that this approximation would be accurate in practical applications since it is currently common to have a very large datasets.


%Once the projections of the particles and the data points are computed along the directions $\{\theta_n\}_n$, it is straightforward to approximately compute the function $(F_{\theta^*_{n\#}\bar{\mu}}^{-1} \circ F_{\theta^*_{n\#}\nu}) $ by using standard techniques that are based on linear interpolation \umut{cite}, whose details are given in the supplementary document.
The overall algorithm is illustrated in Algorithm~\ref{algo:flow}. It is remarkable that the updates of the particles only involves the learning data $\{y_i\}$ through the CDF of its projections on the many $\theta_{nk}\in\Sp^{d-1}$. This has a fundamental consequence of high practical interest: these CDF may be computed in a distributed manner that is independent of the sliced Wasserstein flow. We considered two scenarios in our experiment: they may first be computed \textit{beforehand}, if the $\theta_{nk}$ are always picked from the same subset of the sphere, which we call a \textit{batch} mode, as opposed to the \textit{streaming} mode, where new directions $\theta_{nk}$ are drawn each time.

Then, significant reductions in computing time may also be obtained when the CDF $F_{\theta^*_\#\nu}$ for the target is only computed on random batches from the data, instead of the whole dataset of size $P$. This simplified procedure also has some interesting consequences: since we may control the number of $\theta$ for which a given data point $y_i$ is used by SWF through its projections $\langle\theta,y_i\rangle$, we may guarantee that these projections do not allow to recover $y_i$, by simply picking fewer than necessary for reconstruction, as predicted by the compressed sensing theory \cite{donoho2009observed}.
